{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcec5d06",
   "metadata": {},
   "source": [
    "# 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531b5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "from scipy import stats #Analysis \n",
    "from scipy.stats import norm \n",
    "from pickle import dump\n",
    "\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5d0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas option 설정 하는 코드\n",
    "# monthly_gain의 경우 부동소수점으로 나타나서 보기 어려울땐 윗 줄의 주석을 제거하고 아래에 주석을 추가하고\n",
    "# 다시 원래대로 돌리고 싶다면 아래에 주석제거, 위 코드에 주석추가\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "#pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb15c670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>...</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>hand</th>\n",
       "      <th>religion</th>\n",
       "      <th>orientation</th>\n",
       "      <th>voted</th>\n",
       "      <th>married</th>\n",
       "      <th>familysize</th>\n",
       "      <th>ASD</th>\n",
       "      <th>nerdiness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>20</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>49</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>18</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>14995</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>14996</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>45</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>14997</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>14998</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>14999</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index   Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  ...  engnat  age  \\\n",
       "0          0 1.00 5.00 5.00 5.00 1.00 4.00 5.00 5.00 1.00  ...    1.00   20   \n",
       "1          1 4.00 4.00 4.00 4.00 4.00 5.00 4.00 4.00 3.00  ...    1.00   49   \n",
       "2          2 4.00 5.00 5.00 4.00 3.00 5.00 5.00 5.00 4.00  ...    2.00   43   \n",
       "3          3 4.00 4.00 4.00 2.00 4.00 3.00 3.00 5.00 3.00  ...    1.00   17   \n",
       "4          4 4.00 4.00 4.00 4.00 3.00 3.00 4.00 2.00 3.00  ...    2.00   18   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...  ...   \n",
       "14995  14995 2.00 5.00 4.00 3.00 3.00 4.00 4.00 4.00 3.00  ...    1.00   17   \n",
       "14996  14996 5.00 4.00 5.00 4.00 4.00 5.00 5.00 4.00 4.00  ...    2.00   45   \n",
       "14997  14997 4.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 4.00  ...    1.00   20   \n",
       "14998  14998 5.00 5.00 4.00 5.00 5.00 5.00 5.00 1.00 5.00  ...    1.00   29   \n",
       "14999  14999 5.00 4.00 2.00 5.00 2.00 2.00 4.00 2.00 4.00  ...    2.00   21   \n",
       "\n",
       "       hand  religion  orientation  voted  married  familysize  ASD  nerdiness  \n",
       "0      2.00     12.00         4.00   2.00     1.00        4.00 2.00          1  \n",
       "1      1.00      2.00         1.00   1.00     2.00        4.00 2.00          1  \n",
       "2      1.00      2.00         2.00   2.00     3.00        4.00 2.00          1  \n",
       "3      2.00      1.00         1.00   2.00     1.00        2.00 2.00          1  \n",
       "4      2.00     12.00         1.00   2.00     1.00        1.00 2.00          0  \n",
       "...     ...       ...          ...    ...      ...         ...  ...        ...  \n",
       "14995  1.00      1.00         3.00   2.00     1.00        3.00 2.00          0  \n",
       "14996  1.00      3.00         1.00   1.00     2.00        3.00 2.00          1  \n",
       "14997  1.00      1.00         2.00   1.00     1.00        3.00 1.00          1  \n",
       "14998  1.00     12.00         4.00   2.00     2.00        2.00 1.00          0  \n",
       "14999  1.00      2.00         2.00   2.00     1.00        1.00 2.00          1  \n",
       "\n",
       "[15000 rows x 70 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./Dataset/\"\n",
    "\n",
    "data = pd.read_csv(path+\"train.csv\")\n",
    "\n",
    "# original data -> data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecf56c",
   "metadata": {},
   "source": [
    "# 2. EDA \n",
    "\n",
    "15,000 rows × 70 columns\n",
    "\n",
    "- feature (1)\n",
    "    - index \n",
    "- Categorical variable (63)\n",
    "    - country : 137 values, not valanced \n",
    "    - education : 4 values \n",
    "    - urban : 4 values, but start with 0 \n",
    "    - gender : 3 \n",
    "    - engnat : 2 \n",
    "    - hand : 3 \n",
    "    - religion : 12 \n",
    "    - orientation : 5\n",
    "    - voted : 2\n",
    "    - married : 3\n",
    "    - ASD : 2 \n",
    "    - Q(26) : 5 values\n",
    "        - 1=Disagree, 2=Slightly disagree, 3=Neutral, 4=Slightly agree, 5=Agree\n",
    "    - TIPI(10) : 5 values \n",
    "        - items were rated \"I see myself as:\" _____ such that Extraverted, enthusiastic...\n",
    "    - VCL(16) : 2 values (0 or 1) \n",
    "        - no more need for preprocessing\n",
    "- Numerical variable (5)\n",
    "    - introelapse : 1315 values \n",
    "    - testelapse : 684 values \n",
    "    - surveyelapse : 712 values\n",
    "    - age : 76 values, but maybe outlier...\n",
    "    - familysize : 20 values, but maybe outlier... \n",
    "- Y variable (1)\n",
    "    - nerdiness \n",
    "        - binary classification \n",
    "        - 1:8303, 0:6697"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a75e361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 70 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   index         15000 non-null  int64  \n",
      " 1   Q1            14959 non-null  float64\n",
      " 2   Q2            14931 non-null  float64\n",
      " 3   Q3            14950 non-null  float64\n",
      " 4   Q4            14929 non-null  float64\n",
      " 5   Q5            14962 non-null  float64\n",
      " 6   Q6            14952 non-null  float64\n",
      " 7   Q7            14924 non-null  float64\n",
      " 8   Q8            14952 non-null  float64\n",
      " 9   Q9            14944 non-null  float64\n",
      " 10  Q10           14928 non-null  float64\n",
      " 11  Q11           14941 non-null  float64\n",
      " 12  Q12           14933 non-null  float64\n",
      " 13  Q13           14960 non-null  float64\n",
      " 14  Q14           14964 non-null  float64\n",
      " 15  Q15           14955 non-null  float64\n",
      " 16  Q16           14967 non-null  float64\n",
      " 17  Q17           14963 non-null  float64\n",
      " 18  Q18           14937 non-null  float64\n",
      " 19  Q19           14947 non-null  float64\n",
      " 20  Q20           14955 non-null  float64\n",
      " 21  Q21           14961 non-null  float64\n",
      " 22  Q22           14962 non-null  float64\n",
      " 23  Q23           14950 non-null  float64\n",
      " 24  Q24           14939 non-null  float64\n",
      " 25  Q25           14956 non-null  float64\n",
      " 26  Q26           14932 non-null  float64\n",
      " 27  country       14810 non-null  object \n",
      " 28  introelapse   15000 non-null  int64  \n",
      " 29  testelapse    15000 non-null  int64  \n",
      " 30  surveyelapse  15000 non-null  int64  \n",
      " 31  TIPI1         14947 non-null  float64\n",
      " 32  TIPI2         14934 non-null  float64\n",
      " 33  TIPI3         14921 non-null  float64\n",
      " 34  TIPI4         14936 non-null  float64\n",
      " 35  TIPI5         14930 non-null  float64\n",
      " 36  TIPI6         14938 non-null  float64\n",
      " 37  TIPI7         14936 non-null  float64\n",
      " 38  TIPI8         14935 non-null  float64\n",
      " 39  TIPI9         14936 non-null  float64\n",
      " 40  TIPI10        14920 non-null  float64\n",
      " 41  VCL1          15000 non-null  int64  \n",
      " 42  VCL2          15000 non-null  int64  \n",
      " 43  VCL3          15000 non-null  int64  \n",
      " 44  VCL4          15000 non-null  int64  \n",
      " 45  VCL5          15000 non-null  int64  \n",
      " 46  VCL6          15000 non-null  int64  \n",
      " 47  VCL7          15000 non-null  int64  \n",
      " 48  VCL8          15000 non-null  int64  \n",
      " 49  VCL9          15000 non-null  int64  \n",
      " 50  VCL10         15000 non-null  int64  \n",
      " 51  VCL11         15000 non-null  int64  \n",
      " 52  VCL12         15000 non-null  int64  \n",
      " 53  VCL13         15000 non-null  int64  \n",
      " 54  VCL14         15000 non-null  int64  \n",
      " 55  VCL15         15000 non-null  int64  \n",
      " 56  VCL16         15000 non-null  int64  \n",
      " 57  education     14833 non-null  float64\n",
      " 58  urban         15000 non-null  int64  \n",
      " 59  gender        14981 non-null  float64\n",
      " 60  engnat        14953 non-null  float64\n",
      " 61  age           15000 non-null  int64  \n",
      " 62  hand          14953 non-null  float64\n",
      " 63  religion      14755 non-null  float64\n",
      " 64  orientation   14601 non-null  float64\n",
      " 65  voted         14915 non-null  float64\n",
      " 66  married       14918 non-null  float64\n",
      " 67  familysize    14681 non-null  float64\n",
      " 68  ASD           14911 non-null  float64\n",
      " 69  nerdiness     15000 non-null  int64  \n",
      "dtypes: float64(46), int64(23), object(1)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d8412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# missing value check\n",
    "msno.matrix(data.iloc[:, :], color=(0.1, 0.6, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c9722",
   "metadata": {},
   "source": [
    "# 3. Preprocessing\n",
    "data -> processed_data (**15,000 x 70 -> 15,000 x 50**)\n",
    "\n",
    "Basic \n",
    "- **Drop unnecessary features** \n",
    "    - index, country(...)\n",
    "- **Handling missing values (...)**\n",
    "    - **fillna 0**\n",
    "        - education, gender, hand, religion, orientation, voted, married, ASD\n",
    "        - 무응답은 0으로 채우기 (urban은 이미 0부터 시작) \n",
    "    - **imputation : simple method**\n",
    "        \n",
    "Question (Q, TIPI, VCL)\n",
    "- TIPI scoring to 'O', 'C', 'E', 'A', 'N' \n",
    "- VCL scoring (he words at **VCL6, VCL9, and VCL12** are not real words and can be used as a validity check)\n",
    "\n",
    "    \n",
    "Numerical\n",
    "- **Log Transformation**\n",
    "    - 3 elapse cols (intro, test, survey)\n",
    "- **Ranging**\n",
    "    - 'age' to 10 category (0~10/10~20..90~100)\n",
    "- **Handling outlier**\n",
    "    - 'familysize' to make np.Nan and imputate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923478ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 data와 따로 관리 -> original data = data, preprocessed data = processed_data \n",
    "# Feature drop \n",
    "#'index', 'country'\n",
    "processed_data = data.drop(['index', 'country'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72df48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>engnat</th>\n",
       "      <th>hand</th>\n",
       "      <th>religion</th>\n",
       "      <th>orientation</th>\n",
       "      <th>voted</th>\n",
       "      <th>married</th>\n",
       "      <th>ASD</th>\n",
       "      <th>nerdiness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>15000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.31</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.19</td>\n",
       "      <td>4.10</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       education   gender   engnat     hand  religion  orientation    voted  \\\n",
       "count   15000.00 15000.00 15000.00 15000.00  15000.00     15000.00 15000.00   \n",
       "mean        2.31     1.70     1.35     1.19      4.10         1.85     1.63   \n",
       "std         0.92     0.55     0.47     0.48      3.40         1.21     0.48   \n",
       "min         1.00     1.00     1.00     1.00      1.00         1.00     1.00   \n",
       "25%         2.00     1.00     1.00     1.00      2.00         1.00     1.00   \n",
       "50%         2.00     2.00     1.00     1.00      2.00         1.00     2.00   \n",
       "75%         3.00     2.00     2.00     1.00      6.00         2.00     2.00   \n",
       "max         4.00     3.00     2.00     3.00     12.00         5.00     2.00   \n",
       "\n",
       "       married      ASD  nerdiness  \n",
       "count 15000.00 15000.00   15000.00  \n",
       "mean      1.18     1.94       0.55  \n",
       "std       0.46     0.24       0.50  \n",
       "min       1.00     1.00       0.00  \n",
       "25%       1.00     2.00       0.00  \n",
       "50%       1.00     2.00       1.00  \n",
       "75%       1.00     2.00       1.00  \n",
       "max       3.00     2.00       1.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling missing values with simple imputator\n",
    "# 'education', 'gender', 'hand', 'religion', 'orientation', 'voted', 'married', 'ASD'\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "fill_cols = ['education', 'gender', 'engnat', 'hand', 'religion', 'orientation', 'voted', 'married', 'ASD', 'nerdiness']\n",
    "transformer = SimpleImputer()\n",
    "processed_data[fill_cols] = transformer.fit_transform(processed_data[fill_cols])\n",
    "processed_data[fill_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9350c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAFACAYAAAC/TO3EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxb0lEQVR4nO3deXwUZbr//W9naRJBQshCJCib4ug4ooiyyKICDsfljKLgBmEREQMvRzk6MurPcY7Oy3FXUIRRExJAUUB0QCcuqDCCiAZxBDdAQRII2UgAydq5nz94uk4n6ax3km7w8/4rna6+66Kq7qvq211pXMYYIwAAAABAs4UEugAAAAAAONYRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACyFNfUFe/fubY06Gi02Nlb5+fkBrcErWGoJljokagnmOiT/tXTt2jVA1bSsQPcmr2Da3xL1NCTY6pGCr6ZA1XO89Cap7fpTsB07EjU1FjU1TrDUVFd/4hMrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAAS2GBLsCfFStWKDs72+9z4eHhqqioqPX7vLw8SVJcXFyL1pKYmKhrrrmmRccEgGBQs9fW1UfpgwCawre3hIeHa+/evZKq9xb6Co5HQRmssrOztWfHdnVxVdV6rryO15Saox++lR8sbLE69hs+0ANw/KrZa/31UfoggKby7S3lqt1b6Cs4XgVlsJKkLq4q3eQua/TyS8rbSVKTXtPYMQHgeOXba/31UfoggOaor7fQV3C84i0DAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAASwQrAAAAALBEsAIAAAAAS60SrFasWKEVK1a0xtCwwH4Bjk/BOreDtS4ATRPouRzo9QONFdYag2ZnZ7fGsLDEfgGOT8E6t4O1LgBNE+i5HOj1A43FrYAAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYCks0AWg7WRnZ6ukpES33357oEux1qlTJ5WUlGjq1KlasWKF9u/fL2OM83zHjh116NAhjR07VitXrlRFRYVGjRqlDz74QMYYRUREqLS0VO3bt1dFRYViY2NVVVWlAwcO6NJLL9WqVauc5aOiolRUVOS3jiuvvFKjRo1ScXGx5s+fr/z8fN1www165ZVXJEl33nmnYmNjJUnFxcVauHChJk+eLGOMFi5cqN///vdKSUnR9ddfr6VLl+qPf/yjEhMTm71diouLtWDBAuXl5emOO+6wGut45LsPOnbs2KwxsrKyNGfOHOt91VS+tW/fvl1paWmSpNDQUHk8Hp100kltVktj7dixQ5KCoue4XC5nLrtcrmr9QpJiY2MVFhamAwcOKC4uTjfddJOWL1/uzNGbb75ZGRkZGjp0qNLS0pSQkKAhQ4Zo2bJlio+P1+TJk7V06VJVVFSooKBA119/vV599VVJ0tSpU/Xuu+/q2muv1fLly6v1gNGjR+vFF19UVVWVXC6XunTponvvvbfWservsbfn+Jvr/vqN73Hv7zhu6vzwHcMY43e8l156SZJ0yy236ODBgwGZO8cKf3N80qRJ+uWXX7Rs2TL169dPmzdv1rhx4zRkyBB99913euGFFxQWFqZp06bprbfeUn5+vnOs1rUfi4uLNWfOHOXl5WnSpEnq169frRp8z02vvPKKKioqArrPbHpJVFSUiouL/T7XoUMHHT58WJLUvn17RUZGKj8/X1FRUTp48KDCw8N17rnn6rPPPnNeExYWpsGDB2vdunXq3LmzZs2apb179+qFF17QxIkT9e677yonJ0fJyclKSEjQSy+9JJfLpXHjxmnhwoXKzc1VeHi4brzxRr322mu15kNWVpaeffZZxcXFafr06dXmr+/PGzdu1OrVq3XllVfqggsu0IIFC7R//36nj9xwww1asWJFreOgZm9YsGCBcnNzFR8fr+nTpzf63NgS59PmrOPAgQN69tlnW3y9LXVu5xOrX5GSkpJAl9BiioqKVFZWppSUFOXk5NS6SDp48KCMMXr99ddVUVEhSXr//fed5UpLSyVJv/zyi8rLy7V3717l5OSorKxMq1atqrZ8XaFKkrNsRkaGsrOzVVZWpkWLFqm8vFzl5eXOxa93mR9//FEZGRnOz6mpqSotLdWiRYtUWlpabfnmyMjIUFZWlsrKyqzHOh757oPmSk9Pb5F91VS+tS9evNj5vcfjkSTl5OS0aT3HGt+5XLNfSFJ+fr7TA7KyspSWllZtjqakpOjHH3/UokWLZIzRvn37tGzZMklSbm6u0tLStHv3bu3du1dlZWVavHix0wdSU1P1448/OmP69oCUlBSVl5ersrJSFRUVysrK0htvvFHrWPX32Ntz/B2L/vqN73Hv7zhu6vzwHaOu8Xbv3q3du3crIyMjYHPnWOFvji9atMg5zjZv3ixJev311yVJqampMsaooqJCKSkpysrKqnas1rUfMzIylJeX54zvrwbfc1N5ebmMMdq/f3+r/LtbW12hSpITqqSj1wP5+fnOa4wxKi8vrxaqJKmyslLr1q2TJBUWFiojI8PZF4sWLdK+fftkjFFKSoozB3bt2qX09HTl5uZKkioqKrR48WK/8yE9Pd3pQzXnr+/Pq1evlnT0OsR77q+oqFB5ebn27Nmj9PR0v8dBzfGysrJUXl7urK+xWuJ82px1rFy5slXW21L9iWD1K5GamhroElpFMITFVatWaePGjc5j74WudPRid/fu3SouLtamTZtkjNHGjRv12WefyRjj1O97cZydnd2sOoqLi6udAGzGOh757oPPPvtMBw8ebPIYWVlZToBpy+3rW/v69eurHWNexhh9//33bVJPYwTDp1Q2vG/YeOdoSUmJjDF+t713eV++y3lf6x3TXw/w9dFHHznPf/bZZ8rOzq527GZnZ1frOTWPxbr6jfe493ccN3V+1BzD33i+/WjDhg0BmTvHCt/tv2HDBuf4qet4W758ebVjp+bPde3H4uJibdiwwXns8XicwOZbQ81zk3Q0DARiv911111tvs6m+OSTT/xur5KSEn366afO47p6hO988J1X0tF5452/vnP5k08+qTaW73p811fzOKjZG3z7iHecxpwbW+J82px1FBcXa+3atS2+3pY8t7fKrYB5eXkqKyvTnDlzmvX6rKwshRtXC1fVdAeMSxX//0eD/oSHhzufhgRSY+rwfoyOlvf+++/X+/xzzz2nHj16qKqqStLRZurvHXOvtLQ03XvvvU2uIyMjQ5WVlS0y1vEoIyPD2QdVVVXKyMjQuHHjmjRGenp6tcdttX19a6/v2Jk3b5569+5d7Xf19YfG9NqG+iCarrKyUi5X3dvd4/E4z1dVVSktLa3asZuWllbrgtv3WPQ9Xnz7jfe4r3k+SEtLU+/evZs0P2rOBX/j+dboHdtfvajdnxri/cSkPv72Y0ZGRq0esmjRIvXr169aDXV56qmn1L179wbX3VBvaUpfKS8vb3CZYNWYfSkdnQ9PPfVUrXlVVVXl7K/6rh3qW4/vceC7j2teL3jX0ZhzY0ucTxvibx3GmFr9rCXW25Ln9gY/sfrggw80e/ZszZ49u1krAH7tsrOzlZmZ6Vxk1HdhLDX/lq7MzMwWG+tY0NTe5LsPPB6Pvvjiiyavs+b2bKvt61t7fRo6thA8GtpXvhdTOTk51Y5df8ed7+/q6jfe497fcdzU+VHfse8dr75/4/HcmyS7/tRS/O1Hf+cJ73obU0MwvJl8PPL9tKQm7zxqbn/3PQ4as48bc25sifNpc9aRmZnpBMKWXG9Lntsb/MRq5MiRGjlyZJMGjYuLk9T8W0HmzJmj8p2Bv6Ul2mXk7tatzn9HbGyscz9uIDWmjmP9tpxjWWJionr06KGNGzc670TX1yATEhKatZ7zzjtP69evb5GxjgVN7U3nnXeesw9CQ0PVv3//Jq8zISGhWsNtq+3rW3t9IiMja831+vpDY3ptQ32wLvSc+jXUB7zPh4aGKi4uTnl5ec6xGxcXV+vE73ss+h4vvuvxHvc7duyodRz37t27SfOj5lyo+Vzv3r21YcOGOv+Nx3Nvkuz6U0vxtx/9nSdCQ0MbXUNCQkKj5nZDvaUpfeXX0Eu888HfvPLO4YZ6Rl18j4PG7OPGnBtb4nzanHV4bwGsrKxs0fW25Lmdv7H6lTj33HMDXcJxa9SoUc6JyZ+ZM2dq9OjRCgk5Ot1CQ0MVFlb3exoTJ05sVh2jR4+uNW5zxzoe+e6DkJAQjR49usljJCUlVXvcVtvXt/b6biGbMmVKm9QDO2FhYfX2jNDQUOf5kJAQTZw4sdqxO3HixFqv9z0W6+o33uPe33Hc1PlRcwx/4/nW6B3bX72o3Z8aMmzYsAaX8bcfR48eXauHTJgwoVYNdQnEfnO73W2+zpbSmH0p/d92rTmvQkJCnHlU37VDfevxPQ5897G/PhQaGtqoc2NLnE+bsw7f47cl19uS53aC1a/E5MmTA11Cq4iMjAx0Cbryyis1cOBA57Fvo0pISFD37t0VFRWlCy64QC6XSwMHDtSAAQPkcrmc+r2vSUhIaPbXfEZFRWnAgAHV1s1XGv8f330wYMCAZn1Na7du3aq9s9hW29e39gsvvNDvRbnL5dLpp5/eJvU0xrH+N1kJCQnV5mhkZKRcLledgajmO5y+y3lf6x3TXw/wdfHFFzvPDxgwQImJidWO3cTExGo9p+axWFe/8R73/o7jps6PmmP4G8+3Hw0ePDggc+dY4bv9Bw8eXO1i2p9rr7222rFT8+e69mNUVJQGDx7sPA4NDXW+bt23hprnJuno32sGYr898cQTbb7OphgyZIjf7RUZGalBgwY5j+vqEb7zwXdeSUfnjXf++s7lIUOGVBvLdz2+66t5HNTsDb59xDtOY86NLXE+bc46oqKiNHz48BZfb0ue2wlWvyLBEEJaSqdOndSuXTtNmTLFaR6+Onbs6Py/EeHh4ZKOfrLkXS4iIkLS0f+3wu12q2vXrkpISFC7du105ZVXVlu+U6dOddbhXXb06NFKTExUu3btNGHCBLndbrnd7lrvIvfq1ct516VXr16aPHmyIiIiNGHCBEVERFi/Gzh69Gh169ZN7dq14x1hP3z3QXMlJSW1yL5qKt/ax48f7/ze9+SMuvnOZX+f+sXGxjo9oFu3bpo4cWK1OTplyhT16tVLEyZMkMvl0kknnaSxY8dKkuLj4zVx4kR1795dXbt2Vbt27TR+/HinD0yePFm9evVyxvTtAVOmTJHb7VZYWJjCw8PVrVs3jRkzptax6u+xt+f4Oxb99Rvf497fcdzU+eE7Rl3jde/eXd27d3c+KQvE3DlW+JvjEyZMcI4zbwDy/rH+5MmT5XK5FB4erilTpqhbt27VjtW69uPo0aOdP9nwflpVswbfc5Pb7Xb+b6RjUVRUVJ3PdejQwfm5ffv2zv87GRUVJZfLJbfbXe0NAunoJz3eTww7d+6s0aNHO/tiwoQJOumkk+RyuTRlyhRnDvTo0UNJSUmKj4+XdDSkjh8/3u98SEpKcvpQzfnr+/MVV1wh6eh1iPfcHx4eLrfbrZNPPllJSUl+j4Oa43Xr1k1ut9tZX2O1xPm0Oeu4+uqrW2W9LdWfXKaJN2zu3bu3wWW871Ta/o3VTe6yRr9mSXk7SWrSaxozprv36cfF31hJ9vulJWtpC8FSS7DUIfmvpWvXrgGqpmU1pje1hUDs7/rmdmP+xsrbN/310Yb6YFPrCqb5IAVfPVLw1RSoeo6X3iS1XX9qrX3V1OuHhnpLU/tKS1+/BNsck6ipsYKlprr6E59YAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWCJYAQAAAIAlghUAAAAAWAprjUETExNbY1hYYr8Ax6dgndvBWheApgn0XA70+oHGapVgdc0117TGsLDEfgGOT8E6t4O1LgBNE+i5HOj1A43FrYAAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWCFYAAAAAYIlgBQAAAACWwgJdQF32mxAtKW/XpOUlNek1jRnz5BYbDQCCj2+v9ddH6YMAmqO+3kJfwfEqKINVYmJinc+Fh4eroqKi1u8j8vIkSe64uBar4+QGagGAY1nN/uavj9IHATSVb88IDw9XxN69kv6vt9BXcLwKymB1zTXX1PlcbGys8vPz27AaADg+1ddrAaC5fHsL1234NeFvrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAEsEKAAAAACwRrAAAAADAkssYYwJdBAAAAAAcy465T6xmz54d6BIcwVJLsNQhUYs/wVKHFFy1HK+CbRtTT/2CrR4p+GoKtnpQt2DcV9TUONTUOMFYk69jLlgBAAAAQLAhWAEAAACApWMuWI0cOTLQJTiCpZZgqUOiFn+CpQ4puGo5XgXbNqae+gVbPVLw1RRs9aBuwbivqKlxqKlxgrEmX3x5BQAAAABYOuY+sQIAAACAYBMW6AIaKz8/X88//7yKiorkcrk0cuRIXXbZZQGrp6qqSrNnz1bnzp0D+g0lv/zyi+bPn689e/bI5XLptttuU58+fdq8jtWrV+vDDz+Uy+XSySefrOTkZLnd7jZZ97x587R582ZFRUXpySeflCQdPnxYTz/9tPLy8hQXF6c777xTHTp0CEgtixYtUmZmpsLCwtSlSxclJyerffv2bV6H1z//+U8tXrxYL730kjp27NiqdfxaBFt/8gqWPuUVLP3KK5B9Swqu3lVXPYHoX6hfY/rNtm3b9Nhjjyk+Pl6SNGDAAF177bWtWteMGTMUERGhkJAQhYaG6u9//3u1540xSk1N1Zdffql27dopOTlZvXr1arV69u7dq6efftp5nJubq3Hjxunyyy93ftcW28lmnm/ZskWpqamqqqrSiBEjdNVVV7VaTY2d6w3t55as6fXXX9eaNWuca5UbbrhB/fr1q/Xa1tpOzWKOEYWFhWbnzp3GGGOOHDlibr/9drNnz56A1bNq1SrzzDPPmEceeSRgNRhjzNy5c80HH3xgjDGmoqLCHD58uM1rKCgoMMnJyaasrMwYY8yTTz5pPvroozZb/7Zt28zOnTvNrFmznN8tWrTIrFy50hhjzMqVK82iRYsCVsuWLVtMZWWlU1db1OKvDmOMycvLMw8//LC57bbbTHFxcavX8WsRbP3JK1j6lFcw9CuvQPctY4Krd9VVTyD6F+rXmH6zdevWNp/3ycnJ9Z5XMjMzzd/+9jdTVVVlvv/+e/PnP/+5zWrzeDxm6tSpJjc3t9rv22I7NXeeezweM3PmTJOTk2MqKirMXXfd1WLnFZu53tB+bsmaXnvtNfPWW2/V+7rW3E7NcczcChgdHe28sxEZGanExEQVFhYGpJaCggJt3rxZI0aMCMj6vY4cOaJvv/1Wl1xyiSQpLCwsYO8kVlVVqby8XB6PR+Xl5YqOjm6zdZ955pm13un5/PPPNXz4cEnS8OHD9fnnnweslr59+yo0NFSS1KdPnzY5bv3VIUlpaWm66aab5HK5Wr2GX5Ng6k9ewdKnvIKpX3kFsm9JwdW76qonEP0L9QvGftMYX3zxhYYNGyaXy6U+ffrol19+0YEDB9pk3V9//bUSEhIUFxfXJuvz1dx5vmPHDiUkJKhLly4KCwvT4MGDW6wfBONcr+u6pSGtuZ2a45i5FdBXbm6ufvrpJ5166qkBWf/ChQs1fvx4lZSUBGT9Xrm5uerYsaPmzZun3bt3q1evXpo0aZIiIiLatI7OnTvryiuv1G233Sa3262+ffuqb9++bVpDTcXFxc5FUnR0tA4ePBjQerw+/PBDDR48OCDr/uKLL9S5c2f16NEjIOv/tQh0f/IKlj7lFSz9yisY+5YUvL1LCmz/gn/19ZsffvhBd999t6KjozVhwgSdfPLJrV7P3/72N0nSqFGjan17W2FhoWJjY53HMTExKiwsbJM3NNavX68LL7zQ73OB2E6NmeeFhYWKiYlxHsfExGj79u2tXpvU8Fyvbz+3tHfffVfr1q1Tr169lJSUVCt8BXI7+XPMfGLlVVpaqieffFKTJk3SCSec0Obrz8zMVFRUVKveF9xYHo9HP/30ky699FI99thjateund588802r+Pw4cP6/PPP9fzzz2vBggUqLS3VunXr2ryOYPfGG28oNDRUQ4cObfN1l5WV6Y033tB1113X5uv+NQl0f/IKpj7lFSz9you+1TSB7F/wr75+07NnT82bN0+PP/64Ro8erccff7zV63nooYf06KOP6t5779W7776rb775ptrzxs+XULfF3ROVlZXKzMzUwIEDaz0XiO3UWIHaXg3N9Yb2c0u69NJLNXfuXD322GOKjo5Wenp6rWUCtZ3qckwFq8rKSj355JMaOnSoBgwYEJAavv/+e33xxReaMWOGnnnmGW3dulVz5swJSC0xMTGKiYnRaaedJkkaOHCgfvrppzav4+uvv1Z8fLw6duyosLAwDRgwQD/88EOb1+ErKirKucXgwIEDAf+Sho8//liZmZm6/fbbAzLh9+/fr9zcXN19992aMWOGCgoKdM8996ioqKjNazleBUN/8gqmPuUVLP3KKxj7lhR8vUsKfP9CbQ31mxNOOMH5NLhfv37yeDyt/uln586dJR09hs8//3zt2LGj2vMxMTHKz893HhcUFLTJp1VffvmlevbsqU6dOtV6LhDbSWrcPI+JiVFBQYHzuC22V2PmekP7uSV16tRJISEhCgkJ0YgRI7Rz585aywRiO9XnmAlWxhjNnz9fiYmJuuKKKwJWx4033qj58+fr+eef1x133KGzzjpLt99+e0Bq6dSpk2JiYrR3715JRy8UunXr1uZ1xMbGavv27SorK5MxRl9//bUSExPbvA5f/fv319q1ayVJa9eu1fnnnx+wWrZs2aK33npL99xzj9q1axeQGk455RS99NJLev755/X8888rJiZGjz76qN8TDZouWPqTVzD1Ka9g6Vdewdi3pODqXVJw9C9U15h+U1RU5LyTv2PHDlVVVenEE09stZpKS0ud245LS0v1n//8R6ecckq1Zfr3769169bJGKMffvhBJ5xwQsBvA2zr7eTVmHneu3dv7du3T7m5uaqsrNSGDRvUv3//VqupMXO9Mfu5Jfn+Dd6mTZv83qbZ1tupIcfMfxD83Xff6YEHHtApp5zipOi6vnaxrWzbtk2rVq0K6NcY79q1S/Pnz1dlZaXi4+OVnJzcZl/N6+v111/Xhg0bFBoaqh49emj69OkKDw9vk3U/88wz+uabb3To0CFFRUVp3LhxOv/88/X0008rPz9fsbGxmjVrVptsF3+1rFy5UpWVlc76TzvtNE2bNq3N6/B+aYB09OtSH3nkkaB4N/x4EIz9ySsY+pRXsPQrr0D2LSm4eldd9QSif6F+dfUb76dBl156qTIyMvTee+8pNDRUbrdbSUlJOv3001utpv379+uJJ56QdPS23yFDhmjMmDF67733nJqMMXr55Zf11Vdfye12Kzk5Wb179261mqSjt8Hfdttteu6555zbJX1raovt1JR5XlhYqAULFujPf/6zJGnz5s1KS0tTVVWVLr74Yo0ZM6bVaqprrvvWVNd+bq2atm3bpl27dsnlcikuLk7Tpk1TdHR0m22n5jhmghUAAAAABKtj5lZAAAAAAAhWBCsAAAAAsESwAgAAAABLBCsAAAAAsESwAgAAAABLBKsgNGvWLG3bti3QZejBBx/UmjVrAl0GAEg6+tXx06dPD3QZANAsM2bM0H/+859Al4FWRLAKQk899ZR++9vfNrgcExRAoLRE//n444/1//7f/2uhigAACCyC1XHM4/EEugQAAICgwbURWlNYoAtAbTNmzNCtt96q7777TllZWXK73dq0aZNiY2M1Y8YM9e7dW3PnzlV+fr4effRRhYSE6Nprr9WgQYM0c+ZMTZ8+XcuWLVN8fLz+8pe/aOXKlVqzZo3Ky8t1zjnnaMqUKc7/QP7DDz8oPT1dWVlZiouL06RJk/x+WpaTk6MFCxZo9+7dcrlc6tu3r26++Wa1b9/eqXnkyJFat26dioqKdP7552vq1Klyu906ePCg5s2bp++++04ul0snn3yyHnzwQYWEhKiwsFApKSn69ttvFRERocsvv1yXXXZZm25vAE3jr/+cccYZdfaSjz/+WMuXL9fBgwd14okn6vrrr1fPnj314osvqrKyUhMmTFBoaKgWLlyoiooKvfrqq/r0009VWVmp888/X5MmTZLb7a5Vx5tvvqk1a9aouLhYMTExuuGGG3TBBRc461yzZo169uyptWvXKjo6WjfffLN+97vf1VnT0KFDJUkffvihVq1apaKiIp166qmaNm2a4uLi2mjrAqjPm2++qX/9618qKSlRdHS0pk6dqnXr1ikmJkbXX3+9pKO3Dc+dO1fz58+XdPQaZdSoUfrkk0+0d+9eXXPNNdq1a5f+53/+xxk3NTVVxhhNmTJFR44cUVpamr788ku5XC5dfPHFGjdunDwej6ZNm6a//vWvOuWUUyRJxcXFSk5O1gsvvKCOHTsqMzNTS5cuVV5enrp166ZbbrlF3bt3r/Xv2LFjh1JTU5WdnS23260BAwZo4sSJCgs7emk+btw4TZo0Se+8845KSkp00UUX6aabblJISIhycnL0wgsvaNeuXQoLC9NZZ52lO++8U5KUnZ2tlJQU/fjjj+rYsaOuu+46DR48uFX3CXwYBJ3k5GTz1Vdfmddee83ceOONJjMz03g8HrNkyRJz77331lrOa//+/Wbs2LFm7ty5pqSkxJSVlZk1a9aYmTNnmpycHFNSUmIef/xxM2fOHGOMMQUFBWby5MnO+F999ZWZPHmyKS4uNsYY85e//MV88MEHxhhj9u3bZ7766itTXl5uiouLzQMPPGBSU1Or1TJr1iyTl5dnDh06ZO6//37z6quvGmOMWbJkiVmwYIGpqKgwFRUV5ptvvjFVVVXG4/GYP/3pT2bZsmWmoqLC5OTkmBkzZpgvv/yylbcwAFu+/ae+XlJSUmKSkpJMdna2McaYwsJC8/PPPxtjjPnoo4/M/fffX23c1NRU8/e//90cOnTIHDlyxDzyyCNmyZIlxhhjtm7dam699VZn2Q0bNpiCggLj8XjM+vXrzfjx401hYaEz9nXXXWdWrVplKioqzPr1601SUpI5dOhQvTV99tlnZubMmWbPnj2msrLSLF++3Nx3332tuCUBNFZ2draZPn26KSgoMMYcve7Zt2+fee6555xrDmNq94rk5GRz1113mby8PFNWVmZyc3PNTTfdZH755RdjjDEej8fccsst5vvvvzfGGPPoo4+aBQsWmJKSElNUVGRmz55t3nvvPWOMMS+++KJZtGiRM/bbb79tHnnkEWOMMTt37jQ333yz+eGHH4zH4zEfffSRSU5ONuXl5U4d3r65c+dO8/3335vKykqzf/9+c8cdd5jVq1c7444dO9Y8+OCD5tChQyYvL8/cfvvtzjXZ008/bVasWGE8Ho8pKysz3377rTHGmJKSEjN9+nTz4YcfmsrKSrNz504zZcoUp7+h9XErYJD7zW9+o379+ikkJETDhg3Trl27GnzN2LFjFRERIbfbrU8++URXXHGFunTpooiICN14443asGGDPB6P1q1bp3PPPdcZ/+yzz1bv3r21efPmWmMmJCTo7LPPVnh4uDp27KjLL79c33zzTbVlfv/73ys2NlYdOnTQ1VdfrfXr10uSQkNDVVRUpPz8fIWFhemMM86Qy+XSzp07dfDgQV177bUKCwtTly5dNGLECG3YsKFFth2AttFQL3G5XPr5559VXl6u6OhonXzyyX7HMcZozZo1mjhxojp06KDIyEiNGTPG6SU1DRo0SJ07d1ZISIgGDx6shIQE7dixw3k+KipKl19+ucLCwjR48GB17dq1wZo++OADXX311erWrZtCQ0N19dVXa9euXcrLy2vJTQagGUJCQlRRUaGsrCxVVlYqPj5eCQkJjXrtf/3Xfyk2NlZut1txcXHq2bOnPv/8c0nS1q1b1a5dO/Xp00dFRUXasmWLJk2apIiICKePeK9Nhg8frvXr16uqqkrS0f43bNgwSdKaNWs0cuRInXbaaQoJCdFFF12ksLAwbd++vVY9vXr1Up8+fRQaGqr4+HiNHDmy1nXVH/7wB3Xo0EGxsbG67LLLnF4YFhamvLw8HThwQG63W7/5zW8kSZs3b1ZcXJwuvvhihYaGqlevXhowYIA2btzYjK2N5uBWwCAXFRXl/Ox2u1VRUSGPx6PQ0NA6XxMTE+P8fODAgWq3sMTGxsrj8ai4uFj5+fnauHGjMjMznec9Ho/fWwGLi4uVmpqqb7/9VqWlpaqqqlKHDh2qLRMbG+v8HBcXp8LCQknSf//3f2vZsmV6+OGHJUkjR47UVVdd5TSFSZMmOa+rqqrSGWec0dBmARBE6uslERERuuOOO7Rq1SrNnz9fp59+upKSkpSYmFhrnIMHD6qsrEyzZ892fmeMcS5galq7dq1Wr17thJ7S0lIdOnTIeb5z585yuVzOY29fqq+mvLw8paamKj09vVoNhYWF3A4IBFhCQoImTZqkZcuWKSsrS3379lVSUlKjXut7jSJJQ4YM0fr16zV8+HB98sknuvDCCyUd7WfeW/68jDHOtdVpp52mdu3a6ZtvvlF0dLRycnLUv39/57Vr165VRkaG89rKykrnesjX3r17lZ6erp07d6q8vFwej0e9evWqtozv9VxcXJwOHDggSRo/fryWLl2qe++9V+3bt9cVV1yhSy65RHl5edq+fXu16yqPx+MEP7Q+gtVxyPdCIjo6uto7rfn5+QoNDVVUVJRiYmI0dOjQRn198SuvvCJJeuKJJ3TiiSdq06ZNSklJqbZMfn5+tZ87d+4sSYqMjFRSUpKSkpK0Z88e/fWvf1Xv3r0VGxur+Ph4zZkzx+rfCyCwGuol55xzjs455xyVl5dr6dKlWrBggf73f/+31nInnnii3G63nnrqKad/1CUvL08LFizQAw88oD59+igkJER33323jDHOMoWFhTLGOD0xPz/fuQCqq6bY2FiNGTPG+XsrAMFlyJAhGjJkiI4cOaJ//OMfWrJkiSIjI1VWVuYsU1RU1OA4gwYNUnp6ugoKCrRp0ybnzd+YmBiFhYXp5ZdfrvNN7OHDh+vf//63OnXqpIEDBzp/AxoTE6MxY8ZozJgxDa7/pZdeUo8ePfTHP/5RkZGRevvtt2t9slRQUOB8mp6fn6/o6GhJUqdOnZx++9133+mhhx7SmWeeqZiYGJ155pl822oAcSvgMaxTp07Kzc2td5kLL7xQb7/9tnJzc1VaWqpXX31VgwYNUmhoqIYOHarMzExt2bJFVVVVKi8v17Zt21RQUFBrnJKSEkVERKh9+/YqLCzUqlWrai3z7rvvqqCgQIcPH9bKlSs1aNAgSVJmZqZycnJkjFFkZKRCQkIUEhKiU089VZGRkXrzzTdVXl6uqqoq/fzzz9Vu5QEQnHz7T329pKioSF988YVKS0sVFhamiIgIhYSEOGMUFhaqsrJS0tHbfEaMGKGFCxequLhY0tFwtGXLllrrLysrk8vlUseOHSVJH330kfbs2VNtmeLiYv3rX/9SZWWlPv30U2VnZ+vcc8+tt6ZRo0bpzTffdMY6cuSIPv3005bfgACabO/evdq6dasqKirkdrvldrsVEhKiHj166Msvv9Thw4dVVFSkd955p8GxOnbsqN/+9reaN2+e4uPj1a1bN0lH35Du27ev0tPTdeTIEVVVVSknJ6fabXrDhg3Tpk2b9O9//7vap0EjRozQ+++/r+3bt8sYo9LSUm3evFklJSW11l9SUqITTjhBERERys7O1nvvvVdrmX/+8586fPiw8vPz9c477zhfQvHpp58612reLxELCQnReeedp3379mndunWqrKxUZWWlduzYoaysrCZsZdjgE6tj2FVXXaWUlBQtXrxYY8aM0cCBA2stc/HFF+vAgQP6y1/+ovLycvXt21dTpkyRdPRj8T/96U9avHixnn32WSfs3HLLLbXGGTt2rJ577jlNnDhRCQkJGjZsmN5+++1qywwZMkQPP/ywDhw4oP79++uaa66RJO3bt08pKSk6ePCg2rdvr0svvdS53fCee+5Renq6ZsyYocrKSnXt2lXXXXddS28qAC2sZv+pq5cYY7Rq1SrNnTtXLpdLPXr00NSpUyVJZ511lvOtWSEhIXr55Zd10003afny5brvvvt06NAhde7cWaNGjdI555xTbf3dunXTFVdcofvuu8/5G9TTTz+92jKnnXaa9u3bp5tvvlmdOnXSrFmzdOKJJ+rAgQN11nTBBReotLRUzzzzjPLz83XCCSfod7/7nfNGEYDAqaio0JIlS5Sdna3Q0FCdfvrpmjZtmjp06KCvv/5aM2bMUFxcnC666CKtXr26wfGGDBmi5557TuPHj6/2+5kzZ2rJkiWaNWuWSkpK1KVLF/3hD39wno+JiVGvXr2Uk5NT7c8XevfurVtvvVUpKSnat2+f8/dP/v7EYcKECfrHP/6ht956Sz179tTgwYO1devWasv0799fs2fP1pEjR3TRRRfpkksukSTt3LlTCxcu1JEjR9SpUydNnjxZ8fHxkqT7779faWlpSktLkzFG3bt318SJExu/kWHFZXzvmwCayfsV8WeffXagSwEA5+vWH3rooUCXAuA4NG/ePHXu3Nn5iveWNm7cOM2ZM6fRX86B4MCtgAAAAEAj5ebmatOmTc4nSIAXtwICAAAAjbB06VK9/fbbuvrqq53b7wAvbgUEAAAAAEvcCggAAAAAlghWAAAAAGCJYAUAAAAAlghWAAAAAGCJYAUAAAAAlghWAAAAAGDp/wMJtEMUI4oBZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log Transformation\n",
    "# 'introelapse','testelapse','surveyelapse'\n",
    "right_skewed_cols = ['introelapse','testelapse','surveyelapse']\n",
    "for c in right_skewed_cols : \n",
    "    processed_data[c] = np.log1p(processed_data[c])\n",
    "\n",
    "# for checking \n",
    "plt.style.use(\"ggplot\") \n",
    "plt.figure(figsize=(15,5))\n",
    "    \n",
    "for i in range(len(right_skewed_cols)):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    sns.boxplot(x=right_skewed_cols[i], data=processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f188e0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00    6824\n",
       "1.00    5161\n",
       "2.00    1493\n",
       "3.00     822\n",
       "4.00     459\n",
       "5.00     202\n",
       "6.00      28\n",
       "9.00       7\n",
       "7.00       3\n",
       "8.00       1\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranging and ordinary encoding \n",
    "# 'age'\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "bins= [10,20,30,40,50,60,70,80,90,100,40000]\n",
    "labels = [10,20,30,40,50,60,70,80,90,100]\n",
    "processed_data['age'] = pd.cut(processed_data['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "processed_data['age'] = oe.fit_transform(processed_data['age'].values.reshape(-1,1))\n",
    "processed_data['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926cf5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count   15000.00\n",
      "mean        2.30\n",
      "std         0.84\n",
      "min         1.00\n",
      "25%         2.00\n",
      "50%         2.00\n",
      "75%         3.00\n",
      "max         4.00\n",
      "Name: familysize, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='familysize'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Handling outlier and iterative imputation\n",
    "# 'familysize'\n",
    "Q1 = processed_data['familysize'].quantile(0.25)\n",
    "Q2 = processed_data['familysize'].quantile(0.5)\n",
    "Q3 = processed_data['familysize'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_fence = Q1 - (1.5 * IQR)\n",
    "upper_fence = Q3 + (1.5 * IQR)\n",
    "processed_data['familysize'] = processed_data['familysize'].apply(lambda x : np.nan if x < lower_fence or x > upper_fence else x)\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "transformer =  IterativeImputer()\n",
    "processed_data[['familysize', 'nerdiness']] = transformer.fit_transform(processed_data[['familysize', 'nerdiness']])\n",
    "print(processed_data['familysize'].describe())\n",
    "sns.boxplot(x='familysize', data=processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c01dff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdkAAAJACAYAAABxH2+WAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdB0lEQVR4nO3dfayXdf3H8TfgDaTI8HBrDAKHk5slKWeWUmSt1HmzX1mHPwxczlzL5k1/HVy6kFpp1H66yK3GKnArwRs2we2M5oiKTrBfHBpgiDNOYNwECmJxK/z+cJyf5wfm4ZV4VB6Pv9jne13X9/09139Prn2uHkeOHDlSAAAAAADACevZ3QMAAAAAAMB7lcgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAIHRadw/wTmttba1169bVxo0bq729vfbu3VuTJk2q22+/vbtHAwAAAADgPeaUi+yPP/54tbe3V+/evauhoaFefPHF7h4JAAAAAID3qFMust90003V0NBQQ4YMqXXr1tWMGTO6eyQAAAAAAN6jTrnIPn78+O4eAQAAAACA9wkvPgUAAAAAgJDIDgAAAAAAoVNuu5i3y3k/+Z//+BqPXXtBVVV9YdFz75prmclMZjKTmcxkJjOZyUxmMpOZzGQmM5np/XGd9/tMR/391kvelutwYm58ekPtPvBaLfqvC7t7lG7nSXYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgNBp3T3AO23FihW1cuXKqqratWtXVVVt2LChZs+eXVVVffv2rWnTpnXXeAAAAAAAvIeccpF948aN9Zvf/KbT2rZt22rbtm1VVTVw4ECRHQAAAACALjnlIntTU1M1NTV19xgAAAAAALwP2JMdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCIjsAAAAAAIREdgAAAAAACInsAAAAAAAQEtkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABAS2QEAAAAAICSyAwAAAABASGQHAAAAAICQyA4AAAAAACGRHQAAAAAAQiI7AAAAAACERHYAAAAAAAiJ7AAAAAAAEBLZAQAAAAAgJLIDAAAAAEBIZAcAAAAAgJDIDgAAAAAAIZEdAAAAAABCp3XloNbW1lq3bl1t3Lix2tvba+/evTVp0qS6/fbbjzl2+/bt9fWvf/1Nr3XZZZfVnXfeedzPli5dWi0tLbV58+bq2bNnjRw5sq677rq65JJLjnv8gQMHauHChfX73/++duzYUX369KmxY8dWU1NTDRs27Ljn7Ny5sx599NFavXp17dmzp/r371+NjY31hS98oc4+++y3/mMAAAAAAPwbW7Zsqebm5nr66adrz549NWrUqHr44Ydr8uTJVVX16quv1vTp0+vJJ5+snTt31vDhw+urX/1q3XXXXd08+fvbybovXYrsjz/+eLW3t1fv3r2roaGhXnzxxbc8Z8SIEdXY2HjM+vDhw497/Ny5c2vRokXV0NBQn/70p+vQoUO1fPnyuv/+++vmm2+uq666qtPxBw8erJkzZ9b69evr/PPPr6uvvrp27txZra2ttWrVqrr33ntr9OjRnc7ZunVr3XPPPbV79+6aOHFiffCDH6znn3++nn766Wpra6uZM2dW3759u/InAQAAAAA4xq5du+ryyy+vSZMm1eLFi2vgwIH1wgsv1KBBgzqO+cY3vlG//vWva968eTVy5MhatmxZfeUrX6kBAwbU1KlTu3H696+TeV+6FNlvuummamhoqCFDhtS6detqxowZb3nOhz70oWpqaurK5Wv9+vW1aNGiGjx4cH33u9/teKL8+uuvr+bm5po3b15dfPHFnX7wokWLav369fXRj3607rzzzurZ8/Wdby677LL6/ve/Xw8//HDNmjWrY72qas6cObV79+768pe/XFdffXXH+i9+8YtavHhx/fKXv6xbb721SzMDAAAAAPx/DzzwQA0dOrTmzp3bsTZy5MhOxyxfvrymTp1aV1xxRVW93lLnzJlTf/zjH98Tkf3AocP1wu599dK+Q/X9lS/WHR8ZWmec9u7emfxk3pcu/fLx48fX0KFDq0ePHsn8b2nJkiVVVfX5z3++05YtgwYNqiuvvLIOHjxYS5cu7Vg/cuRIxzlf+tKXOoX0xsbGGjNmTG3evLnWrVvXsb5t27ZavXp1DRw4sK688spO39/U1FRnnnlm/fa3v619+/adjJ8IAAAAAJwCFi5cWJdeemlNmTKlBg0aVBMmTKgf/ehHdeTIkY5jJk2aVE899VRt2rSpql6Pu21tbcfs5vFudODQ4ZrwyJ/rb3sO1KsHD9d/r9paEx75cx04dLi7R/u3TuZ9OWn/vfDyyy/XkiVL6oknnqglS5ZUe3v7mx67Zs2aqqqaMGHCMZ995CMf6XRM1evBfMeOHTV06NBOT7cfdfQ6bzzn6L8vuuiiTlG+qqpPnz514YUX1v79+2vDhg1d+4EAAAAAAP/PCy+8UD/+8Y9r1KhR1dLSUnfccUc1NzfX7NmzO4556KGHasKECTV8+PA6/fTTa/LkyXX//ffXtdde242Td82Dq7bU7gOvdVrbfeC1enDVlm6aqGtO5n3pceSNqb4L1q5dWzNmzIhefDpu3Li67bbbasCAAR1r+/btq2nTplXv3r07Pap/1CuvvFK33HJL9evXr376059WVdWf/vSn+t73vlcXX3xxNTc3H3NOa2tr/fCHP6yPfexjHZvSz5s3r5566qmaOnVqXXfddcecM2fOnGppaalbbrmlPvvZz3btjwEAAAAA8AZnnHFGTZw4sZYvX96xdvfdd9eTTz5Zzz77bFVV/eAHP6if/OQnNWvWrBoxYkQtW7asmpub67HHHnvXP83etOi5+v3f9xyzPum8vvXotRd0w0RdczLvS5f2ZD8RZ555Zt1www3V2NhYgwcPrqqq9vb2WrBgQa1du7buu+++euCBB6p3795VVfWvf/2rqqo+8IEPHPd6R9f/+c9/dqx19Zyjx6XfAwAAAABwIoYOHVpjx47ttDZmzJh68MEHq6pq7969NX369FqwYEHHw8Af/vCHq62trWbNmvWuj+zz38Uh/d85mfflbd8upl+/fjVlypQaNWpUnXXWWXXWWWfV2LFj65vf/GaNHj26tm7dWs8888wJX/dE9oM/wYfzO51zsvadBwAAAADe/y6//PJav359p7XnnnuuRowYUVVVBw8erIMHD1avXr06HdOrV686fPjdva/5e9nJvC/v2Ctfe/XqVZ/61Keqqjq9kPR4T52/0fGeQH+rc/bu3fu2nAMAAAAAcCLuuuuuam1tre985zv1/PPP14IFC+qhhx6q2267raqqzjnnnJo8eXI1NzfX0qVL669//Wv9/Oc/r7lz59bnPve5bp7+/etk3pe3fbuYf+ecc86pqqr9+/d3rPXu3bvOPffceumll+rll1+u/v37dzpn69atVfX64/xHnXfeeVVVtWXL8TfTP7p+Iucc73sAAAAAAE5EY2NjLVy4sO6+++6aOXNmDR8+vGbOnFlf+9rXOo751a9+VdOnT68bb7yxXnrppRoxYkTNnDnzTd91yX/uZN6XdzSyb9iwoaqqY6/2o8aPH1/Lli2rtra2uuKKKzp9tmrVqo5jjho8eHANGDCgtmzZUtu3b69BgwZ1Oqetre2Yc8aNG1dVVatXr67Dhw9Xz57/9xD/3r176y9/+UudccYZNXr06P/wVwIAAAAAp7Jrrrmmrrnmmjf9fMiQIfWzn/3sHZyIqpN3X9727WI2bNhQhw4dOmZ9zZo1tXjx4qqq+vjHP97ps8985jNVVfXEE0/Uq6++2rG+ffv2amlpqdNPP70++clPdqz36NGj45xHHnmk0544K1eurGeffbaGDRvWaSP7IUOG1EUXXVT/+Mc/qqWlpdP3z58/v/bv31+f+MQnOl7ICgAAAAAAb6XHkS68JXTFihW1cuXKqqratWtXrV69ugYPHlwXXnhhVVX17du3pk2bVlVV3/rWt2rTpk01bty4Ovfcc6uq6m9/+1utWbOmqqqmTJlSN9xwwzHfMXfu3Fq0aFE1NDTUpZdeWocOHao//OEPtWfPnrr55puPeXvrwYMH67777qv169fX+eefX+PHj68dO3ZUa2trnXbaaXXvvfce81T61q1b65577qndu3fXxIkTa9iwYbVhw4Zau3ZtDR06tL797W9X3759T/RvCAAAAADAKapLkX3+/Pn12GOPvennAwcOrNmzZ1dV1TPPPFMrVqyoTZs21SuvvFKvvfZa9evXry644IK66qqrasyYMW96naVLl1ZLS0tt3ry5evToUSNHjqzrr7++LrnkkuMef+DAgVq4cGH97ne/qx07dlSfPn1q3Lhx1dTUVMOGDTvuOTt27Kj58+dXW1tb7dmzp/r371+NjY31xS9+sc4+++y3+lMAAAAAAECHLkV2AAAAAADgWG/7nuwAAAAAAHCqENkBAAAAACAksgMAAAAAQEhkBwAAAACAkMgOAAAAAAAhkR0AAAAAAEIiOwAAAAAAhER2AAAAAAAIiewAAAAAABD6Xw73Dy8fBYdLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finish handling missing values! \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "npas_cols = ['Q{}'.format(i) for i in range(1,27)]\n",
    "tipi_cols = ['TIPI{}'.format(i) for i in range(1,11)]\n",
    "vcl_cols = ['VCL{}'.format(i) for i in range(1,17)]\n",
    "mach_cols = ['Q{}'.format(i) for i in range(1,21)]\n",
    "q_cols = [npas_cols, tipi_cols, vcl_cols]\n",
    "\n",
    "for col in q_cols : \n",
    "    transformer = SimpleImputer()\n",
    "    processed_data[col] = transformer.fit_transform(processed_data[col])\n",
    "    \n",
    "msno.matrix(processed_data.iloc[:, :], color=(0.1, 0.6, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7155f1",
   "metadata": {},
   "source": [
    "the Nerdy Personality Attributes Scale\n",
    "\n",
    "Your score was 30. Scores range from a low of 30 to a high of 70. The exact average score is 50. People who score higher on the NPAS are more likely to identify as nerds. Below is a graph of what percent of people say yes when asked the question \"Are you a nerd?\" based on what their NPAS score was.\n",
    "\n",
    "http://openpsychometrics.org/tests/NPAS/development/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03f30c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3.05\n",
       "1       3.10\n",
       "2       3.00\n",
       "3       2.95\n",
       "4       2.95\n",
       "        ... \n",
       "14995   3.15\n",
       "14996   2.80\n",
       "14997   3.00\n",
       "14998   2.65\n",
       "14999   2.95\n",
       "Name: Mach, Length: 15000, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_mach(data) :\n",
    "    rev_cols = ['Q3', 'Q4', 'Q7', 'Q9', 'Q10', 'Q11', 'Q16', 'Q17', 'Q14', 'Q18']\n",
    "    for flip in rev_cols: \n",
    "        data[flip] = 6 - data[flip]\n",
    "    data['Mach'] = data[mach_cols].mean(axis=1)\n",
    "    #data.drop(mach_cols, axis=1, inplace = True)\n",
    "    return data \n",
    "\n",
    "processed_data = score_mach(processed_data)\n",
    "processed_data['Mach']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e62415",
   "metadata": {},
   "source": [
    "TIPI scale scoring (“R” denotes reverse-scored items):\n",
    "\n",
    "\n",
    "\n",
    "- Extraversion: 1, 6R; \n",
    "\n",
    "- Agreeableness: 2R, 7; \n",
    "\n",
    "- Conscientiousness; 3, 8R; \n",
    "\n",
    "- Emotional Stability: 4R, 9;\n",
    "\n",
    "- Openness to Experiences: 5, 10R.\n",
    "\n",
    "coring the TIPI\n",
    "\n",
    "1. Recode the reverse-scored items (i.e., recode a 7 with a 1, a 6 with a 2, a 5 with a 3, etc.). The reverse scored items are 2, 4, 6, 8, & 10.\n",
    "\n",
    "2. Take the AVERAGE of the two items (the standard item and the recoded reverse-scored item) that make up each scale.\n",
    "\n",
    "Example using the Extraversion scale: A participant has scores of 5 on item 1 (Extraverted, enthusiastic) and and 2 on item 6 (Reserved, quiet). First, recode the reverse-scored item (i.e., item 6), replacing the 2 with a 6. Second, take the average of the score for item 1 and the (recoded) score for item 6. So the TIPI Extraversion scale score would be: (5 + 6)/2 = 5.5\n",
    "\n",
    "https://gosling.psy.utexas.edu/scales-weve-developed/ten-item-personality-measure-tipi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf3ab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>2.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>2.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         E    A    C    N    O\n",
       "0     3.50 5.00 5.00 6.00 4.00\n",
       "1     5.00 5.50 5.00 2.50 4.50\n",
       "2     2.00 4.50 3.50 6.00 5.50\n",
       "3     4.00 4.50 3.50 3.50 5.00\n",
       "4     3.50 4.00 4.00 3.50 5.00\n",
       "...    ...  ...  ...  ...  ...\n",
       "14995 2.50 4.50 4.50 4.00 4.00\n",
       "14996 4.00 5.00 5.50 2.50 5.00\n",
       "14997 2.00 4.00 3.50 2.00 4.00\n",
       "14998 2.00 6.00 3.50 2.50 5.50\n",
       "14999 5.00 4.50 3.50 4.00 5.00\n",
       "\n",
       "[15000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_tipi(data) :\n",
    "    data['E'] = (data['TIPI1'] + (8-data['TIPI6'])) / 2\n",
    "    data['A'] = (data['TIPI7'] + (8-data['TIPI2'])) / 2\n",
    "    data['C'] = (data['TIPI3'] + (8-data['TIPI8'])) / 2\n",
    "    data['N'] = (data['TIPI9'] + (8-data['TIPI4'])) / 2\n",
    "    data['O'] = (data['TIPI5'] + (8-data['TIPI10'])) / 2\n",
    "    data.drop(['TIPI1', 'TIPI2', 'TIPI3', 'TIPI4', 'TIPI5', 'TIPI6', 'TIPI7', 'TIPI8', 'TIPI9', 'TIPI10'], axis=1, inplace = True)\n",
    "    return data \n",
    "\n",
    "processed_data = score_tipi(processed_data)\n",
    "processed_data[['E', 'A', 'C', 'N', 'O']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47c421",
   "metadata": {},
   "source": [
    "VCL scale scoring\n",
    "\n",
    "The following items were presented as a check-list and subjects were instructed \"In the grid below, check all the words whose definitions you are sure you know\":\n",
    "\n",
    "A value of 1 is checked, 0 means unchecked. The words at VCL6, VCL9, and VCL12 are not real words and can be used as a validity check.\n",
    "\n",
    "https://www.kaggle.com/datasets/lucasgreenwell/nerdy-personality-attributes-scale-responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3934e448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VCL_score</th>\n",
       "      <th>VCL_faker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>11.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>11.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VCL_score  VCL_faker\n",
       "0           8.00       0.00\n",
       "1          11.00       0.00\n",
       "2          11.00       0.00\n",
       "3           9.00       0.00\n",
       "4           8.00       0.00\n",
       "...          ...        ...\n",
       "14995      10.00       1.00\n",
       "14996       8.00       0.00\n",
       "14997       9.00       0.00\n",
       "14998      11.00       1.00\n",
       "14999      11.00       2.00\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_vcl(data) :\n",
    "    data['VCL_score'] = (data['VCL1'] + data['VCL2'] + data['VCL3'] + data['VCL4'] + data['VCL5'] + data['VCL7'] + data['VCL8']\n",
    "                        + data['VCL10'] + data['VCL11'] + data['VCL13'] + data['VCL14'] + data['VCL15'] + data['VCL16'])\n",
    "    data['VCL_faker'] = (data['VCL6'] + data['VCL9'] + data['VCL12'])\n",
    "    data.drop(['VCL1', 'VCL2', 'VCL3', 'VCL4', 'VCL5', 'VCL6', 'VCL7', 'VCL8', 'VCL9', \n",
    "               'VCL10', 'VCL11', 'VCL12', 'VCL13', 'VCL14', 'VCL15', 'VCL16',], axis=1, inplace = True)\n",
    "    return data \n",
    "\n",
    "processed_data = score_vcl(processed_data)\n",
    "processed_data[['VCL_score', 'VCL_faker']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520471a",
   "metadata": {},
   "source": [
    "# 4. Data Split \n",
    "split data as train/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "249d6915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>ASD</th>\n",
       "      <th>nerdiness</th>\n",
       "      <th>Mach</th>\n",
       "      <th>E</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>VCL_score</th>\n",
       "      <th>VCL_faker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.95</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.15</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.95</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10  ...  ASD  nerdiness  \\\n",
       "0     1.00 5.00 1.00 1.00 1.00 4.00 1.00 5.00 5.00 3.00  ... 2.00       1.00   \n",
       "1     4.00 4.00 2.00 2.00 4.00 5.00 2.00 4.00 3.00 3.00  ... 2.00       1.00   \n",
       "2     4.00 5.00 1.00 2.00 3.00 5.00 1.00 5.00 2.00 2.00  ... 2.00       1.00   \n",
       "3     4.00 4.00 2.00 4.00 4.00 3.00 3.00 5.00 3.00 2.00  ... 2.00       1.00   \n",
       "4     4.00 4.00 2.00 2.00 3.00 3.00 2.00 2.00 3.00 2.00  ... 2.00       0.00   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...        ...   \n",
       "14995 2.00 5.00 2.00 3.00 3.00 4.00 2.00 4.00 3.00 2.00  ... 2.00       0.00   \n",
       "14996 5.00 4.00 1.00 2.00 4.00 5.00 1.00 4.00 2.00 1.00  ... 2.00       1.00   \n",
       "14997 4.00 5.00 1.00 1.00 5.00 5.00 1.00 5.00 2.00 1.00  ... 1.00       1.00   \n",
       "14998 5.00 5.00 2.00 1.00 5.00 5.00 1.00 1.00 1.00 1.00  ... 1.00       0.00   \n",
       "14999 5.00 4.00 4.00 1.00 2.00 2.00 2.00 2.00 2.00 2.00  ... 2.00       1.00   \n",
       "\n",
       "       Mach    E    A    C    N    O  VCL_score  VCL_faker  \n",
       "0      3.05 3.50 5.00 5.00 6.00 4.00       8.00       0.00  \n",
       "1      3.10 5.00 5.50 5.00 2.50 4.50      11.00       0.00  \n",
       "2      3.00 2.00 4.50 3.50 6.00 5.50      11.00       0.00  \n",
       "3      2.95 4.00 4.50 3.50 3.50 5.00       9.00       0.00  \n",
       "4      2.95 3.50 4.00 4.00 3.50 5.00       8.00       0.00  \n",
       "...     ...  ...  ...  ...  ...  ...        ...        ...  \n",
       "14995  3.15 2.50 4.50 4.50 4.00 4.00      10.00       1.00  \n",
       "14996  2.80 4.00 5.00 5.50 2.50 5.00       8.00       0.00  \n",
       "14997  3.00 2.00 4.00 3.50 2.00 4.00       9.00       0.00  \n",
       "14998  2.65 2.00 6.00 3.50 2.50 5.50      11.00       1.00  \n",
       "14999  2.95 5.00 4.50 3.50 4.00 5.00      11.00       2.00  \n",
       "\n",
       "[15000 rows x 50 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리된 data와 따로 관리 -> preprocessed data = procssed_data, input data = input_data \n",
    "input_data = processed_data.copy()\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984c2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_y = input_data['nerdiness'].copy()\n",
    "input_data_X = input_data.drop(['nerdiness'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bfb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# train/ test data 로 split \n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    input_data_X, \n",
    "    input_data_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 7,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e012e",
   "metadata": {},
   "source": [
    "# 5. Neural Network \n",
    "\n",
    "https://dacon.io/competitions/official/235647/codeshare/1812?page=1&dtype=vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921b9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4448e",
   "metadata": {},
   "source": [
    "**Dataset of PyTorch**\n",
    "PyTorch에서 사용되는 Dataset은 학습에 필요한 데이터 sample을 정제하고 label을 저장하는 기능을 제공함  \n",
    "Dataset class에는 초기화 메서드(__init__), 호출 메서드(__getitem__), 길이 반환 메서드(__len__)를 재정의하여 활용함 \n",
    "\n",
    "**DataLoader of PyTorch**\n",
    "PyTorch에서 사용되는 DataLoader는 데이터 세트(Dataset)에 저장된 데이터를 어떠한 방식으로 불러와 활용할지 정의함  \n",
    "배치 크기(Batch Size), 데이터 순서 변경(Shuffle), 데이터 로드 프로세스 수(num_workers) 기능 제공  \n",
    "- 전체 데이터 세트에 대해 배치 크기 만큼 샘플을 나누고 모든 배치를 대상으로 학습을 완료하면 한 번의 epoch 완료 \n",
    "    - 1,000개의 데이터 샘플에 대해 100 batch size를 지정하면 10번의 batch 완료 = 1 epoch \n",
    "- Shuffle을 통해 데이터의 순서로 학습되는 것을 방지함 \n",
    "\n",
    "https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89\n",
    "\n",
    "https://076923.github.io/posts/Python-pytorch-12/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa4e8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPEAT = 5\n",
    "N_SKFOLD = 7\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.05\n",
    "LOADER_PARAM = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30678719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom DataLoaders\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    \n",
    "train_data = TrainData(torch.FloatTensor(train_X.to_numpy()), torch.FloatTensor(train_y.to_numpy()))\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "test_data = TestData(torch.FloatTensor(test_X.to_numpy()))\n",
    "test_dataloader = DataLoader(test_data, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8445ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer dim \n",
    "n_input_dim = train_X.shape[1]\n",
    "n_hidden1 = 294\n",
    "n_hidden2 = 30\n",
    "n_output_dim = 1\n",
    "\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(n_input_dim, n_hidden1) \n",
    "        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.layer_out = nn.Linear(n_hidden2, n_output_dim) \n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.leakyrelu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.leakyrelu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc89682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9736025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassification(\n",
      "  (layer_1): Linear(in_features=49, out_features=294, bias=True)\n",
      "  (layer_2): Linear(in_features=294, out_features=30, bias=True)\n",
      "  (layer_out): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BinaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14ce0a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.57207 | Acc: 70.426\n",
      "Epoch 002: | Loss: 0.55911 | Acc: 71.191\n",
      "Epoch 003: | Loss: 0.55778 | Acc: 71.133\n",
      "Epoch 004: | Loss: 0.55979 | Acc: 71.080\n",
      "Epoch 005: | Loss: 0.55541 | Acc: 71.543\n",
      "Epoch 006: | Loss: 0.55850 | Acc: 71.426\n",
      "Epoch 007: | Loss: 0.55700 | Acc: 71.431\n",
      "Epoch 008: | Loss: 0.55421 | Acc: 71.771\n",
      "Epoch 009: | Loss: 0.55808 | Acc: 71.122\n",
      "Epoch 010: | Loss: 0.55262 | Acc: 71.878\n",
      "Epoch 011: | Loss: 0.55344 | Acc: 71.755\n",
      "Epoch 012: | Loss: 0.55865 | Acc: 71.527\n",
      "Epoch 013: | Loss: 0.55363 | Acc: 71.479\n",
      "Epoch 014: | Loss: 0.55556 | Acc: 71.681\n",
      "Epoch 015: | Loss: 0.55402 | Acc: 71.681\n",
      "Epoch 016: | Loss: 0.55508 | Acc: 71.473\n",
      "Epoch 017: | Loss: 0.55550 | Acc: 71.830\n",
      "Epoch 018: | Loss: 0.55580 | Acc: 71.314\n",
      "Epoch 019: | Loss: 0.55560 | Acc: 71.543\n",
      "Epoch 020: | Loss: 0.55565 | Acc: 71.617\n",
      "Epoch 021: | Loss: 0.55001 | Acc: 72.112\n",
      "Epoch 022: | Loss: 0.55796 | Acc: 71.234\n",
      "Epoch 023: | Loss: 0.55155 | Acc: 71.793\n",
      "Epoch 024: | Loss: 0.55389 | Acc: 72.149\n",
      "Epoch 025: | Loss: 0.54950 | Acc: 72.074\n",
      "Epoch 026: | Loss: 0.54808 | Acc: 71.931\n",
      "Epoch 027: | Loss: 0.55116 | Acc: 72.021\n",
      "Epoch 028: | Loss: 0.54749 | Acc: 72.191\n",
      "Epoch 029: | Loss: 0.54948 | Acc: 72.048\n",
      "Epoch 030: | Loss: 0.55322 | Acc: 71.920\n",
      "Epoch 031: | Loss: 0.55042 | Acc: 72.043\n",
      "Epoch 032: | Loss: 0.54475 | Acc: 72.330\n",
      "Epoch 033: | Loss: 0.54863 | Acc: 71.926\n",
      "Epoch 034: | Loss: 0.54626 | Acc: 72.255\n",
      "Epoch 035: | Loss: 0.54895 | Acc: 72.191\n",
      "Epoch 036: | Loss: 0.54707 | Acc: 72.229\n",
      "Epoch 037: | Loss: 0.54994 | Acc: 72.394\n",
      "Epoch 038: | Loss: 0.54668 | Acc: 72.521\n",
      "Epoch 039: | Loss: 0.54494 | Acc: 71.941\n",
      "Epoch 040: | Loss: 0.54736 | Acc: 71.628\n",
      "Epoch 041: | Loss: 0.54550 | Acc: 72.649\n",
      "Epoch 042: | Loss: 0.55020 | Acc: 72.239\n",
      "Epoch 043: | Loss: 0.54616 | Acc: 72.202\n",
      "Epoch 044: | Loss: 0.54816 | Acc: 72.234\n",
      "Epoch 045: | Loss: 0.54641 | Acc: 72.665\n",
      "Epoch 046: | Loss: 0.54757 | Acc: 72.426\n",
      "Epoch 047: | Loss: 0.54412 | Acc: 72.452\n",
      "Epoch 048: | Loss: 0.54930 | Acc: 71.729\n",
      "Epoch 049: | Loss: 0.54880 | Acc: 71.989\n",
      "Epoch 050: | Loss: 0.54968 | Acc: 72.426\n",
      "Epoch 051: | Loss: 0.54647 | Acc: 72.410\n",
      "Epoch 052: | Loss: 0.54164 | Acc: 72.543\n",
      "Epoch 053: | Loss: 0.54459 | Acc: 72.495\n",
      "Epoch 054: | Loss: 0.54635 | Acc: 72.548\n",
      "Epoch 055: | Loss: 0.55134 | Acc: 72.106\n",
      "Epoch 056: | Loss: 0.54334 | Acc: 72.420\n",
      "Epoch 057: | Loss: 0.54514 | Acc: 72.537\n",
      "Epoch 058: | Loss: 0.54544 | Acc: 71.957\n",
      "Epoch 059: | Loss: 0.54510 | Acc: 72.415\n",
      "Epoch 060: | Loss: 0.54416 | Acc: 72.750\n",
      "Epoch 061: | Loss: 0.54121 | Acc: 72.952\n",
      "Epoch 062: | Loss: 0.54299 | Acc: 72.335\n",
      "Epoch 063: | Loss: 0.54643 | Acc: 72.585\n",
      "Epoch 064: | Loss: 0.54713 | Acc: 72.324\n",
      "Epoch 065: | Loss: 0.54590 | Acc: 72.239\n",
      "Epoch 066: | Loss: 0.54646 | Acc: 72.223\n",
      "Epoch 067: | Loss: 0.54621 | Acc: 72.218\n",
      "Epoch 068: | Loss: 0.54735 | Acc: 72.282\n",
      "Epoch 069: | Loss: 0.54282 | Acc: 72.872\n",
      "Epoch 070: | Loss: 0.54445 | Acc: 72.665\n",
      "Epoch 071: | Loss: 0.54855 | Acc: 72.085\n",
      "Epoch 072: | Loss: 0.54198 | Acc: 72.372\n",
      "Epoch 073: | Loss: 0.54210 | Acc: 72.362\n",
      "Epoch 074: | Loss: 0.54495 | Acc: 72.245\n",
      "Epoch 075: | Loss: 0.54600 | Acc: 72.415\n",
      "Epoch 076: | Loss: 0.54273 | Acc: 71.968\n",
      "Epoch 077: | Loss: 0.54299 | Acc: 72.580\n",
      "Epoch 078: | Loss: 0.54301 | Acc: 72.144\n",
      "Epoch 079: | Loss: 0.54080 | Acc: 72.622\n",
      "Epoch 080: | Loss: 0.54439 | Acc: 72.335\n",
      "Epoch 081: | Loss: 0.54122 | Acc: 72.819\n",
      "Epoch 082: | Loss: 0.54849 | Acc: 72.755\n",
      "Epoch 083: | Loss: 0.54800 | Acc: 72.223\n",
      "Epoch 084: | Loss: 0.54305 | Acc: 72.356\n",
      "Epoch 085: | Loss: 0.53864 | Acc: 72.846\n",
      "Epoch 086: | Loss: 0.54218 | Acc: 72.335\n",
      "Epoch 087: | Loss: 0.53930 | Acc: 72.899\n",
      "Epoch 088: | Loss: 0.54441 | Acc: 72.468\n",
      "Epoch 089: | Loss: 0.54484 | Acc: 72.356\n",
      "Epoch 090: | Loss: 0.53929 | Acc: 72.872\n",
      "Epoch 091: | Loss: 0.54319 | Acc: 72.548\n",
      "Epoch 092: | Loss: 0.54150 | Acc: 72.670\n",
      "Epoch 093: | Loss: 0.54029 | Acc: 72.761\n",
      "Epoch 094: | Loss: 0.53729 | Acc: 72.638\n",
      "Epoch 095: | Loss: 0.53712 | Acc: 73.250\n",
      "Epoch 096: | Loss: 0.53688 | Acc: 72.995\n",
      "Epoch 097: | Loss: 0.54257 | Acc: 72.441\n",
      "Epoch 098: | Loss: 0.53984 | Acc: 72.234\n",
      "Epoch 099: | Loss: 0.53686 | Acc: 72.995\n",
      "Epoch 100: | Loss: 0.53993 | Acc: 72.686\n",
      "Epoch 101: | Loss: 0.54368 | Acc: 72.681\n",
      "Epoch 102: | Loss: 0.54026 | Acc: 72.319\n",
      "Epoch 103: | Loss: 0.53672 | Acc: 72.957\n",
      "Epoch 104: | Loss: 0.53692 | Acc: 73.032\n",
      "Epoch 105: | Loss: 0.53486 | Acc: 72.894\n",
      "Epoch 106: | Loss: 0.53534 | Acc: 72.787\n",
      "Epoch 107: | Loss: 0.54077 | Acc: 72.793\n",
      "Epoch 108: | Loss: 0.53769 | Acc: 73.074\n",
      "Epoch 109: | Loss: 0.53241 | Acc: 73.590\n",
      "Epoch 110: | Loss: 0.53415 | Acc: 73.367\n",
      "Epoch 111: | Loss: 0.53204 | Acc: 73.505\n",
      "Epoch 112: | Loss: 0.53186 | Acc: 73.394\n",
      "Epoch 113: | Loss: 0.53674 | Acc: 72.995\n",
      "Epoch 114: | Loss: 0.53486 | Acc: 73.207\n",
      "Epoch 115: | Loss: 0.53914 | Acc: 72.771\n",
      "Epoch 116: | Loss: 0.53163 | Acc: 73.335\n",
      "Epoch 117: | Loss: 0.53883 | Acc: 72.761\n",
      "Epoch 118: | Loss: 0.53414 | Acc: 73.378\n",
      "Epoch 119: | Loss: 0.53758 | Acc: 72.798\n",
      "Epoch 120: | Loss: 0.53388 | Acc: 73.303\n",
      "Epoch 121: | Loss: 0.52902 | Acc: 73.160\n",
      "Epoch 122: | Loss: 0.53340 | Acc: 73.298\n",
      "Epoch 123: | Loss: 0.53417 | Acc: 72.963\n",
      "Epoch 124: | Loss: 0.53165 | Acc: 73.378\n",
      "Epoch 125: | Loss: 0.53364 | Acc: 73.027\n",
      "Epoch 126: | Loss: 0.53506 | Acc: 73.096\n",
      "Epoch 127: | Loss: 0.53465 | Acc: 73.229\n",
      "Epoch 128: | Loss: 0.53270 | Acc: 72.771\n",
      "Epoch 129: | Loss: 0.53264 | Acc: 73.564\n",
      "Epoch 130: | Loss: 0.52957 | Acc: 73.548\n",
      "Epoch 131: | Loss: 0.53162 | Acc: 73.293\n",
      "Epoch 132: | Loss: 0.52935 | Acc: 73.383\n",
      "Epoch 133: | Loss: 0.52976 | Acc: 73.261\n",
      "Epoch 134: | Loss: 0.53086 | Acc: 73.074\n",
      "Epoch 135: | Loss: 0.53076 | Acc: 73.590\n",
      "Epoch 136: | Loss: 0.53914 | Acc: 72.957\n",
      "Epoch 137: | Loss: 0.53191 | Acc: 73.282\n",
      "Epoch 138: | Loss: 0.52904 | Acc: 73.367\n",
      "Epoch 139: | Loss: 0.52811 | Acc: 73.803\n",
      "Epoch 140: | Loss: 0.52928 | Acc: 73.473\n",
      "Epoch 141: | Loss: 0.52869 | Acc: 73.654\n",
      "Epoch 142: | Loss: 0.52539 | Acc: 73.809\n",
      "Epoch 143: | Loss: 0.53246 | Acc: 73.261\n",
      "Epoch 144: | Loss: 0.52575 | Acc: 73.941\n",
      "Epoch 145: | Loss: 0.52773 | Acc: 73.766\n",
      "Epoch 146: | Loss: 0.53209 | Acc: 73.404\n",
      "Epoch 147: | Loss: 0.52741 | Acc: 74.074\n",
      "Epoch 148: | Loss: 0.53019 | Acc: 73.367\n",
      "Epoch 149: | Loss: 0.52859 | Acc: 73.580\n",
      "Epoch 150: | Loss: 0.52731 | Acc: 73.644\n",
      "Epoch 151: | Loss: 0.52788 | Acc: 73.601\n",
      "Epoch 152: | Loss: 0.52779 | Acc: 73.723\n",
      "Epoch 153: | Loss: 0.52382 | Acc: 74.080\n",
      "Epoch 154: | Loss: 0.52305 | Acc: 73.856\n",
      "Epoch 155: | Loss: 0.52990 | Acc: 73.287\n",
      "Epoch 156: | Loss: 0.53182 | Acc: 73.234\n",
      "Epoch 157: | Loss: 0.52630 | Acc: 73.883\n",
      "Epoch 158: | Loss: 0.52486 | Acc: 73.830\n",
      "Epoch 159: | Loss: 0.52255 | Acc: 74.383\n",
      "Epoch 160: | Loss: 0.52549 | Acc: 73.468\n",
      "Epoch 161: | Loss: 0.52643 | Acc: 73.511\n",
      "Epoch 162: | Loss: 0.52877 | Acc: 72.973\n",
      "Epoch 163: | Loss: 0.52762 | Acc: 73.883\n",
      "Epoch 164: | Loss: 0.52365 | Acc: 73.984\n",
      "Epoch 165: | Loss: 0.52544 | Acc: 74.005\n",
      "Epoch 166: | Loss: 0.52531 | Acc: 73.601\n",
      "Epoch 167: | Loss: 0.52678 | Acc: 73.574\n",
      "Epoch 168: | Loss: 0.52458 | Acc: 74.085\n",
      "Epoch 169: | Loss: 0.52462 | Acc: 74.101\n",
      "Epoch 170: | Loss: 0.52565 | Acc: 73.963\n",
      "Epoch 171: | Loss: 0.52135 | Acc: 74.383\n",
      "Epoch 172: | Loss: 0.52456 | Acc: 73.707\n",
      "Epoch 173: | Loss: 0.52781 | Acc: 73.527\n",
      "Epoch 174: | Loss: 0.51861 | Acc: 74.229\n",
      "Epoch 175: | Loss: 0.52855 | Acc: 73.441\n",
      "Epoch 176: | Loss: 0.52353 | Acc: 74.043\n",
      "Epoch 177: | Loss: 0.52697 | Acc: 73.968\n",
      "Epoch 178: | Loss: 0.52144 | Acc: 74.122\n",
      "Epoch 179: | Loss: 0.52438 | Acc: 74.096\n",
      "Epoch 180: | Loss: 0.52106 | Acc: 74.495\n",
      "Epoch 181: | Loss: 0.52034 | Acc: 74.149\n",
      "Epoch 182: | Loss: 0.52209 | Acc: 74.064\n",
      "Epoch 183: | Loss: 0.52400 | Acc: 73.569\n",
      "Epoch 184: | Loss: 0.52132 | Acc: 73.968\n",
      "Epoch 185: | Loss: 0.51855 | Acc: 74.340\n",
      "Epoch 186: | Loss: 0.52146 | Acc: 74.298\n",
      "Epoch 187: | Loss: 0.52194 | Acc: 73.798\n",
      "Epoch 188: | Loss: 0.52321 | Acc: 73.878\n",
      "Epoch 189: | Loss: 0.52197 | Acc: 74.213\n",
      "Epoch 190: | Loss: 0.51855 | Acc: 73.872\n",
      "Epoch 191: | Loss: 0.52248 | Acc: 74.500\n",
      "Epoch 192: | Loss: 0.52023 | Acc: 74.223\n",
      "Epoch 193: | Loss: 0.52451 | Acc: 73.819\n",
      "Epoch 194: | Loss: 0.52065 | Acc: 73.697\n",
      "Epoch 195: | Loss: 0.51749 | Acc: 74.069\n",
      "Epoch 196: | Loss: 0.51910 | Acc: 74.202\n",
      "Epoch 197: | Loss: 0.51529 | Acc: 74.723\n",
      "Epoch 198: | Loss: 0.51894 | Acc: 74.351\n",
      "Epoch 199: | Loss: 0.51529 | Acc: 74.394\n",
      "Epoch 200: | Loss: 0.52150 | Acc: 73.628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201: | Loss: 0.51279 | Acc: 74.452\n",
      "Epoch 202: | Loss: 0.51843 | Acc: 74.043\n",
      "Epoch 203: | Loss: 0.52311 | Acc: 74.335\n",
      "Epoch 204: | Loss: 0.51495 | Acc: 74.330\n",
      "Epoch 205: | Loss: 0.51500 | Acc: 74.293\n",
      "Epoch 206: | Loss: 0.51103 | Acc: 74.638\n",
      "Epoch 207: | Loss: 0.51156 | Acc: 74.904\n",
      "Epoch 208: | Loss: 0.51522 | Acc: 74.511\n",
      "Epoch 209: | Loss: 0.51616 | Acc: 74.543\n",
      "Epoch 210: | Loss: 0.51397 | Acc: 74.277\n",
      "Epoch 211: | Loss: 0.51747 | Acc: 74.468\n",
      "Epoch 212: | Loss: 0.51612 | Acc: 74.452\n",
      "Epoch 213: | Loss: 0.51101 | Acc: 73.968\n",
      "Epoch 214: | Loss: 0.51133 | Acc: 74.553\n",
      "Epoch 215: | Loss: 0.51853 | Acc: 74.234\n",
      "Epoch 216: | Loss: 0.51563 | Acc: 74.250\n",
      "Epoch 217: | Loss: 0.51212 | Acc: 74.346\n",
      "Epoch 218: | Loss: 0.51604 | Acc: 74.367\n",
      "Epoch 219: | Loss: 0.51415 | Acc: 74.559\n",
      "Epoch 220: | Loss: 0.51658 | Acc: 74.346\n",
      "Epoch 221: | Loss: 0.51469 | Acc: 74.378\n",
      "Epoch 222: | Loss: 0.51307 | Acc: 74.521\n",
      "Epoch 223: | Loss: 0.51304 | Acc: 74.559\n",
      "Epoch 224: | Loss: 0.51327 | Acc: 74.606\n",
      "Epoch 225: | Loss: 0.51075 | Acc: 75.027\n",
      "Epoch 226: | Loss: 0.51208 | Acc: 74.245\n",
      "Epoch 227: | Loss: 0.51059 | Acc: 74.686\n",
      "Epoch 228: | Loss: 0.50943 | Acc: 74.660\n",
      "Epoch 229: | Loss: 0.51150 | Acc: 74.580\n",
      "Epoch 230: | Loss: 0.50442 | Acc: 74.926\n",
      "Epoch 231: | Loss: 0.51514 | Acc: 74.580\n",
      "Epoch 232: | Loss: 0.51287 | Acc: 74.590\n",
      "Epoch 233: | Loss: 0.51238 | Acc: 74.516\n",
      "Epoch 234: | Loss: 0.51342 | Acc: 74.309\n",
      "Epoch 235: | Loss: 0.50956 | Acc: 74.957\n",
      "Epoch 236: | Loss: 0.51226 | Acc: 74.761\n",
      "Epoch 237: | Loss: 0.50561 | Acc: 75.090\n",
      "Epoch 238: | Loss: 0.50930 | Acc: 74.830\n",
      "Epoch 239: | Loss: 0.50933 | Acc: 74.479\n",
      "Epoch 240: | Loss: 0.50833 | Acc: 74.691\n",
      "Epoch 241: | Loss: 0.50273 | Acc: 74.835\n",
      "Epoch 242: | Loss: 0.50737 | Acc: 75.005\n",
      "Epoch 243: | Loss: 0.50981 | Acc: 74.500\n",
      "Epoch 244: | Loss: 0.51556 | Acc: 73.840\n",
      "Epoch 245: | Loss: 0.50665 | Acc: 74.862\n",
      "Epoch 246: | Loss: 0.50772 | Acc: 74.899\n",
      "Epoch 247: | Loss: 0.50703 | Acc: 74.957\n",
      "Epoch 248: | Loss: 0.50436 | Acc: 75.473\n",
      "Epoch 249: | Loss: 0.50545 | Acc: 74.686\n",
      "Epoch 250: | Loss: 0.50413 | Acc: 74.819\n",
      "Epoch 251: | Loss: 0.51031 | Acc: 74.755\n",
      "Epoch 252: | Loss: 0.50402 | Acc: 75.165\n",
      "Epoch 253: | Loss: 0.50271 | Acc: 74.851\n",
      "Epoch 254: | Loss: 0.50587 | Acc: 74.910\n",
      "Epoch 255: | Loss: 0.50460 | Acc: 75.032\n",
      "Epoch 256: | Loss: 0.50195 | Acc: 75.420\n",
      "Epoch 257: | Loss: 0.50715 | Acc: 75.090\n",
      "Epoch 258: | Loss: 0.50221 | Acc: 75.761\n",
      "Epoch 259: | Loss: 0.50489 | Acc: 75.085\n",
      "Epoch 260: | Loss: 0.50499 | Acc: 74.835\n",
      "Epoch 261: | Loss: 0.49954 | Acc: 75.282\n",
      "Epoch 262: | Loss: 0.51017 | Acc: 74.564\n",
      "Epoch 263: | Loss: 0.50118 | Acc: 75.223\n",
      "Epoch 264: | Loss: 0.50693 | Acc: 74.947\n",
      "Epoch 265: | Loss: 0.50088 | Acc: 75.080\n",
      "Epoch 266: | Loss: 0.49793 | Acc: 75.383\n",
      "Epoch 267: | Loss: 0.50731 | Acc: 75.059\n",
      "Epoch 268: | Loss: 0.50437 | Acc: 74.856\n",
      "Epoch 269: | Loss: 0.50104 | Acc: 75.431\n",
      "Epoch 270: | Loss: 0.50003 | Acc: 75.234\n",
      "Epoch 271: | Loss: 0.49894 | Acc: 75.585\n",
      "Epoch 272: | Loss: 0.50262 | Acc: 75.394\n",
      "Epoch 273: | Loss: 0.49984 | Acc: 75.303\n",
      "Epoch 274: | Loss: 0.49835 | Acc: 75.585\n",
      "Epoch 275: | Loss: 0.49869 | Acc: 75.436\n",
      "Epoch 276: | Loss: 0.50307 | Acc: 75.053\n",
      "Epoch 277: | Loss: 0.50511 | Acc: 74.814\n",
      "Epoch 278: | Loss: 0.49644 | Acc: 75.452\n",
      "Epoch 279: | Loss: 0.50040 | Acc: 75.410\n",
      "Epoch 280: | Loss: 0.49362 | Acc: 76.064\n",
      "Epoch 281: | Loss: 0.49654 | Acc: 75.463\n",
      "Epoch 282: | Loss: 0.49531 | Acc: 75.681\n",
      "Epoch 283: | Loss: 0.50126 | Acc: 75.473\n",
      "Epoch 284: | Loss: 0.50000 | Acc: 75.415\n",
      "Epoch 285: | Loss: 0.49668 | Acc: 75.676\n",
      "Epoch 286: | Loss: 0.49660 | Acc: 75.649\n",
      "Epoch 287: | Loss: 0.49599 | Acc: 75.670\n",
      "Epoch 288: | Loss: 0.50144 | Acc: 75.388\n",
      "Epoch 289: | Loss: 0.49902 | Acc: 75.484\n",
      "Epoch 290: | Loss: 0.49545 | Acc: 75.824\n",
      "Epoch 291: | Loss: 0.49795 | Acc: 75.878\n",
      "Epoch 292: | Loss: 0.49467 | Acc: 75.612\n",
      "Epoch 293: | Loss: 0.49274 | Acc: 75.505\n",
      "Epoch 294: | Loss: 0.49318 | Acc: 75.803\n",
      "Epoch 295: | Loss: 0.49165 | Acc: 75.888\n",
      "Epoch 296: | Loss: 0.49513 | Acc: 75.793\n",
      "Epoch 297: | Loss: 0.49673 | Acc: 75.585\n",
      "Epoch 298: | Loss: 0.49700 | Acc: 75.505\n",
      "Epoch 299: | Loss: 0.49464 | Acc: 75.713\n",
      "Epoch 300: | Loss: 0.49480 | Acc: 75.606\n",
      "Epoch 301: | Loss: 0.49190 | Acc: 75.824\n",
      "Epoch 302: | Loss: 0.49544 | Acc: 75.670\n",
      "Epoch 303: | Loss: 0.48584 | Acc: 76.234\n",
      "Epoch 304: | Loss: 0.49237 | Acc: 75.920\n",
      "Epoch 305: | Loss: 0.49500 | Acc: 75.559\n",
      "Epoch 306: | Loss: 0.49170 | Acc: 75.819\n",
      "Epoch 307: | Loss: 0.49336 | Acc: 75.734\n",
      "Epoch 308: | Loss: 0.49456 | Acc: 75.883\n",
      "Epoch 309: | Loss: 0.49338 | Acc: 75.697\n",
      "Epoch 310: | Loss: 0.49242 | Acc: 75.798\n",
      "Epoch 311: | Loss: 0.48988 | Acc: 76.090\n",
      "Epoch 312: | Loss: 0.49120 | Acc: 75.787\n",
      "Epoch 313: | Loss: 0.48766 | Acc: 76.378\n",
      "Epoch 314: | Loss: 0.49589 | Acc: 75.777\n",
      "Epoch 315: | Loss: 0.48781 | Acc: 76.271\n",
      "Epoch 316: | Loss: 0.48722 | Acc: 76.239\n",
      "Epoch 317: | Loss: 0.48726 | Acc: 76.479\n",
      "Epoch 318: | Loss: 0.49403 | Acc: 75.590\n",
      "Epoch 319: | Loss: 0.48665 | Acc: 75.856\n",
      "Epoch 320: | Loss: 0.48962 | Acc: 76.043\n",
      "Epoch 321: | Loss: 0.48838 | Acc: 75.894\n",
      "Epoch 322: | Loss: 0.49039 | Acc: 76.229\n",
      "Epoch 323: | Loss: 0.48750 | Acc: 76.011\n",
      "Epoch 324: | Loss: 0.49045 | Acc: 76.255\n",
      "Epoch 325: | Loss: 0.48798 | Acc: 75.819\n",
      "Epoch 326: | Loss: 0.48313 | Acc: 76.298\n",
      "Epoch 327: | Loss: 0.48615 | Acc: 76.606\n",
      "Epoch 328: | Loss: 0.48982 | Acc: 75.867\n",
      "Epoch 329: | Loss: 0.48299 | Acc: 76.452\n",
      "Epoch 330: | Loss: 0.48218 | Acc: 76.564\n",
      "Epoch 331: | Loss: 0.48128 | Acc: 76.218\n",
      "Epoch 332: | Loss: 0.48284 | Acc: 76.489\n",
      "Epoch 333: | Loss: 0.48499 | Acc: 76.548\n",
      "Epoch 334: | Loss: 0.48574 | Acc: 76.282\n",
      "Epoch 335: | Loss: 0.48049 | Acc: 76.463\n",
      "Epoch 336: | Loss: 0.48300 | Acc: 76.585\n",
      "Epoch 337: | Loss: 0.48826 | Acc: 75.766\n",
      "Epoch 338: | Loss: 0.48216 | Acc: 76.936\n",
      "Epoch 339: | Loss: 0.48350 | Acc: 76.463\n",
      "Epoch 340: | Loss: 0.47671 | Acc: 76.835\n",
      "Epoch 341: | Loss: 0.48267 | Acc: 76.590\n",
      "Epoch 342: | Loss: 0.48118 | Acc: 76.665\n",
      "Epoch 343: | Loss: 0.48108 | Acc: 76.559\n",
      "Epoch 344: | Loss: 0.48185 | Acc: 76.745\n",
      "Epoch 345: | Loss: 0.48137 | Acc: 76.532\n",
      "Epoch 346: | Loss: 0.47851 | Acc: 77.101\n",
      "Epoch 347: | Loss: 0.48510 | Acc: 75.989\n",
      "Epoch 348: | Loss: 0.48217 | Acc: 76.527\n",
      "Epoch 349: | Loss: 0.47986 | Acc: 76.505\n",
      "Epoch 350: | Loss: 0.48535 | Acc: 76.436\n",
      "Epoch 351: | Loss: 0.48029 | Acc: 76.628\n",
      "Epoch 352: | Loss: 0.48155 | Acc: 76.245\n",
      "Epoch 353: | Loss: 0.47873 | Acc: 76.941\n",
      "Epoch 354: | Loss: 0.47930 | Acc: 76.521\n",
      "Epoch 355: | Loss: 0.48208 | Acc: 76.149\n",
      "Epoch 356: | Loss: 0.48094 | Acc: 76.569\n",
      "Epoch 357: | Loss: 0.47809 | Acc: 76.654\n",
      "Epoch 358: | Loss: 0.48504 | Acc: 76.420\n",
      "Epoch 359: | Loss: 0.47842 | Acc: 76.904\n",
      "Epoch 360: | Loss: 0.48396 | Acc: 76.053\n",
      "Epoch 361: | Loss: 0.48122 | Acc: 77.032\n",
      "Epoch 362: | Loss: 0.48145 | Acc: 76.638\n",
      "Epoch 363: | Loss: 0.47628 | Acc: 76.431\n",
      "Epoch 364: | Loss: 0.48100 | Acc: 76.590\n",
      "Epoch 365: | Loss: 0.47546 | Acc: 77.117\n",
      "Epoch 366: | Loss: 0.47916 | Acc: 76.505\n",
      "Epoch 367: | Loss: 0.47915 | Acc: 76.511\n",
      "Epoch 368: | Loss: 0.47716 | Acc: 76.585\n",
      "Epoch 369: | Loss: 0.47840 | Acc: 76.441\n",
      "Epoch 370: | Loss: 0.47620 | Acc: 76.670\n",
      "Epoch 371: | Loss: 0.48075 | Acc: 76.745\n",
      "Epoch 372: | Loss: 0.46923 | Acc: 77.404\n",
      "Epoch 373: | Loss: 0.47920 | Acc: 76.527\n",
      "Epoch 374: | Loss: 0.47582 | Acc: 77.213\n",
      "Epoch 375: | Loss: 0.47898 | Acc: 76.745\n",
      "Epoch 376: | Loss: 0.46759 | Acc: 77.670\n",
      "Epoch 377: | Loss: 0.48030 | Acc: 76.383\n",
      "Epoch 378: | Loss: 0.48039 | Acc: 76.277\n",
      "Epoch 379: | Loss: 0.47701 | Acc: 77.074\n",
      "Epoch 380: | Loss: 0.47366 | Acc: 77.223\n",
      "Epoch 381: | Loss: 0.47856 | Acc: 76.984\n",
      "Epoch 382: | Loss: 0.47598 | Acc: 76.920\n",
      "Epoch 383: | Loss: 0.47989 | Acc: 76.915\n",
      "Epoch 384: | Loss: 0.47447 | Acc: 77.149\n",
      "Epoch 385: | Loss: 0.47684 | Acc: 76.681\n",
      "Epoch 386: | Loss: 0.47317 | Acc: 76.612\n",
      "Epoch 387: | Loss: 0.47581 | Acc: 77.027\n",
      "Epoch 388: | Loss: 0.47175 | Acc: 77.261\n",
      "Epoch 389: | Loss: 0.47416 | Acc: 76.777\n",
      "Epoch 390: | Loss: 0.46790 | Acc: 77.074\n",
      "Epoch 391: | Loss: 0.47673 | Acc: 76.894\n",
      "Epoch 392: | Loss: 0.47987 | Acc: 76.979\n",
      "Epoch 393: | Loss: 0.46948 | Acc: 77.383\n",
      "Epoch 394: | Loss: 0.47320 | Acc: 77.138\n",
      "Epoch 395: | Loss: 0.47257 | Acc: 76.878\n",
      "Epoch 396: | Loss: 0.47829 | Acc: 76.697\n",
      "Epoch 397: | Loss: 0.47245 | Acc: 76.665\n",
      "Epoch 398: | Loss: 0.47296 | Acc: 76.995\n",
      "Epoch 399: | Loss: 0.47595 | Acc: 76.809\n",
      "Epoch 400: | Loss: 0.47206 | Acc: 77.399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401: | Loss: 0.47110 | Acc: 77.181\n",
      "Epoch 402: | Loss: 0.47417 | Acc: 76.798\n",
      "Epoch 403: | Loss: 0.47484 | Acc: 77.090\n",
      "Epoch 404: | Loss: 0.47401 | Acc: 76.739\n",
      "Epoch 405: | Loss: 0.47504 | Acc: 76.787\n",
      "Epoch 406: | Loss: 0.47199 | Acc: 77.005\n",
      "Epoch 407: | Loss: 0.46882 | Acc: 77.755\n",
      "Epoch 408: | Loss: 0.47486 | Acc: 76.771\n",
      "Epoch 409: | Loss: 0.47049 | Acc: 77.170\n",
      "Epoch 410: | Loss: 0.47273 | Acc: 76.824\n",
      "Epoch 411: | Loss: 0.47254 | Acc: 76.787\n",
      "Epoch 412: | Loss: 0.46856 | Acc: 77.160\n",
      "Epoch 413: | Loss: 0.47092 | Acc: 76.920\n",
      "Epoch 414: | Loss: 0.46867 | Acc: 77.138\n",
      "Epoch 415: | Loss: 0.46527 | Acc: 77.484\n",
      "Epoch 416: | Loss: 0.46746 | Acc: 77.670\n",
      "Epoch 417: | Loss: 0.46628 | Acc: 77.628\n",
      "Epoch 418: | Loss: 0.47242 | Acc: 77.059\n",
      "Epoch 419: | Loss: 0.47585 | Acc: 76.559\n",
      "Epoch 420: | Loss: 0.47101 | Acc: 77.340\n",
      "Epoch 421: | Loss: 0.46518 | Acc: 77.798\n",
      "Epoch 422: | Loss: 0.46618 | Acc: 77.069\n",
      "Epoch 423: | Loss: 0.46949 | Acc: 77.351\n",
      "Epoch 424: | Loss: 0.46942 | Acc: 77.239\n",
      "Epoch 425: | Loss: 0.46626 | Acc: 77.814\n",
      "Epoch 426: | Loss: 0.46898 | Acc: 77.319\n",
      "Epoch 427: | Loss: 0.46435 | Acc: 77.665\n",
      "Epoch 428: | Loss: 0.45990 | Acc: 77.809\n",
      "Epoch 429: | Loss: 0.46415 | Acc: 77.378\n",
      "Epoch 430: | Loss: 0.46715 | Acc: 77.771\n",
      "Epoch 431: | Loss: 0.46535 | Acc: 77.824\n",
      "Epoch 432: | Loss: 0.47214 | Acc: 77.574\n",
      "Epoch 433: | Loss: 0.46530 | Acc: 77.824\n",
      "Epoch 434: | Loss: 0.46405 | Acc: 77.894\n",
      "Epoch 435: | Loss: 0.45996 | Acc: 77.713\n",
      "Epoch 436: | Loss: 0.46692 | Acc: 77.319\n",
      "Epoch 437: | Loss: 0.46326 | Acc: 77.644\n",
      "Epoch 438: | Loss: 0.46508 | Acc: 76.894\n",
      "Epoch 439: | Loss: 0.46267 | Acc: 77.394\n",
      "Epoch 440: | Loss: 0.46681 | Acc: 77.447\n",
      "Epoch 441: | Loss: 0.46408 | Acc: 77.622\n",
      "Epoch 442: | Loss: 0.47182 | Acc: 77.340\n",
      "Epoch 443: | Loss: 0.46704 | Acc: 77.452\n",
      "Epoch 444: | Loss: 0.46626 | Acc: 77.872\n",
      "Epoch 445: | Loss: 0.46225 | Acc: 77.904\n",
      "Epoch 446: | Loss: 0.46637 | Acc: 77.170\n",
      "Epoch 447: | Loss: 0.45819 | Acc: 78.218\n",
      "Epoch 448: | Loss: 0.46322 | Acc: 77.755\n",
      "Epoch 449: | Loss: 0.46221 | Acc: 77.984\n",
      "Epoch 450: | Loss: 0.46500 | Acc: 77.346\n",
      "Epoch 451: | Loss: 0.46334 | Acc: 77.649\n",
      "Epoch 452: | Loss: 0.47251 | Acc: 76.926\n",
      "Epoch 453: | Loss: 0.46440 | Acc: 77.484\n",
      "Epoch 454: | Loss: 0.45927 | Acc: 77.878\n",
      "Epoch 455: | Loss: 0.46353 | Acc: 77.543\n",
      "Epoch 456: | Loss: 0.45887 | Acc: 77.697\n",
      "Epoch 457: | Loss: 0.45424 | Acc: 78.335\n",
      "Epoch 458: | Loss: 0.46545 | Acc: 77.660\n",
      "Epoch 459: | Loss: 0.46160 | Acc: 78.090\n",
      "Epoch 460: | Loss: 0.46008 | Acc: 77.793\n",
      "Epoch 461: | Loss: 0.46769 | Acc: 77.574\n",
      "Epoch 462: | Loss: 0.45975 | Acc: 77.681\n",
      "Epoch 463: | Loss: 0.46374 | Acc: 77.245\n",
      "Epoch 464: | Loss: 0.45714 | Acc: 78.383\n",
      "Epoch 465: | Loss: 0.46358 | Acc: 77.596\n",
      "Epoch 466: | Loss: 0.45643 | Acc: 77.809\n",
      "Epoch 467: | Loss: 0.45877 | Acc: 77.856\n",
      "Epoch 468: | Loss: 0.46322 | Acc: 77.824\n",
      "Epoch 469: | Loss: 0.45663 | Acc: 78.356\n",
      "Epoch 470: | Loss: 0.46047 | Acc: 77.521\n",
      "Epoch 471: | Loss: 0.46473 | Acc: 77.612\n",
      "Epoch 472: | Loss: 0.46513 | Acc: 77.543\n",
      "Epoch 473: | Loss: 0.45928 | Acc: 77.941\n",
      "Epoch 474: | Loss: 0.46070 | Acc: 78.207\n",
      "Epoch 475: | Loss: 0.46683 | Acc: 77.303\n",
      "Epoch 476: | Loss: 0.45777 | Acc: 77.782\n",
      "Epoch 477: | Loss: 0.45735 | Acc: 78.484\n",
      "Epoch 478: | Loss: 0.45303 | Acc: 78.128\n",
      "Epoch 479: | Loss: 0.45301 | Acc: 78.117\n",
      "Epoch 480: | Loss: 0.46148 | Acc: 77.809\n",
      "Epoch 481: | Loss: 0.45291 | Acc: 78.229\n",
      "Epoch 482: | Loss: 0.45553 | Acc: 77.931\n",
      "Epoch 483: | Loss: 0.45577 | Acc: 78.016\n",
      "Epoch 484: | Loss: 0.45430 | Acc: 78.277\n",
      "Epoch 485: | Loss: 0.46013 | Acc: 77.840\n",
      "Epoch 486: | Loss: 0.45925 | Acc: 78.074\n",
      "Epoch 487: | Loss: 0.46202 | Acc: 77.856\n",
      "Epoch 488: | Loss: 0.46074 | Acc: 78.250\n",
      "Epoch 489: | Loss: 0.45825 | Acc: 78.064\n",
      "Epoch 490: | Loss: 0.46130 | Acc: 77.729\n",
      "Epoch 491: | Loss: 0.46043 | Acc: 77.809\n",
      "Epoch 492: | Loss: 0.45691 | Acc: 78.223\n",
      "Epoch 493: | Loss: 0.45648 | Acc: 77.596\n",
      "Epoch 494: | Loss: 0.45395 | Acc: 78.495\n",
      "Epoch 495: | Loss: 0.45591 | Acc: 78.064\n",
      "Epoch 496: | Loss: 0.45826 | Acc: 77.936\n",
      "Epoch 497: | Loss: 0.45753 | Acc: 78.160\n",
      "Epoch 498: | Loss: 0.45673 | Acc: 78.191\n",
      "Epoch 499: | Loss: 0.45243 | Acc: 78.431\n",
      "Epoch 500: | Loss: 0.45630 | Acc: 78.170\n",
      "Epoch 501: | Loss: 0.46274 | Acc: 77.644\n",
      "Epoch 502: | Loss: 0.46471 | Acc: 77.431\n",
      "Epoch 503: | Loss: 0.45467 | Acc: 78.085\n",
      "Epoch 504: | Loss: 0.45235 | Acc: 78.463\n",
      "Epoch 505: | Loss: 0.45805 | Acc: 77.750\n",
      "Epoch 506: | Loss: 0.46273 | Acc: 77.628\n",
      "Epoch 507: | Loss: 0.45591 | Acc: 78.080\n",
      "Epoch 508: | Loss: 0.45280 | Acc: 78.436\n",
      "Epoch 509: | Loss: 0.45777 | Acc: 78.340\n",
      "Epoch 510: | Loss: 0.45907 | Acc: 78.117\n",
      "Epoch 511: | Loss: 0.45146 | Acc: 78.787\n",
      "Epoch 512: | Loss: 0.45984 | Acc: 77.750\n",
      "Epoch 513: | Loss: 0.45795 | Acc: 77.872\n",
      "Epoch 514: | Loss: 0.45707 | Acc: 78.170\n",
      "Epoch 515: | Loss: 0.45637 | Acc: 78.096\n",
      "Epoch 516: | Loss: 0.45104 | Acc: 78.404\n",
      "Epoch 517: | Loss: 0.45635 | Acc: 77.920\n",
      "Epoch 518: | Loss: 0.45345 | Acc: 78.404\n",
      "Epoch 519: | Loss: 0.45402 | Acc: 78.356\n",
      "Epoch 520: | Loss: 0.45279 | Acc: 78.489\n",
      "Epoch 521: | Loss: 0.45299 | Acc: 78.457\n",
      "Epoch 522: | Loss: 0.45414 | Acc: 77.915\n",
      "Epoch 523: | Loss: 0.45069 | Acc: 78.223\n",
      "Epoch 524: | Loss: 0.45397 | Acc: 78.495\n",
      "Epoch 525: | Loss: 0.45177 | Acc: 78.372\n",
      "Epoch 526: | Loss: 0.46054 | Acc: 78.277\n",
      "Epoch 527: | Loss: 0.45658 | Acc: 78.154\n",
      "Epoch 528: | Loss: 0.45286 | Acc: 78.223\n",
      "Epoch 529: | Loss: 0.45914 | Acc: 77.888\n",
      "Epoch 530: | Loss: 0.44869 | Acc: 78.771\n",
      "Epoch 531: | Loss: 0.45359 | Acc: 78.229\n",
      "Epoch 532: | Loss: 0.45729 | Acc: 77.989\n",
      "Epoch 533: | Loss: 0.45107 | Acc: 77.984\n",
      "Epoch 534: | Loss: 0.45023 | Acc: 78.489\n",
      "Epoch 535: | Loss: 0.45130 | Acc: 78.399\n",
      "Epoch 536: | Loss: 0.45851 | Acc: 78.128\n",
      "Epoch 537: | Loss: 0.45009 | Acc: 77.957\n",
      "Epoch 538: | Loss: 0.45463 | Acc: 78.319\n",
      "Epoch 539: | Loss: 0.45994 | Acc: 78.266\n",
      "Epoch 540: | Loss: 0.44934 | Acc: 78.638\n",
      "Epoch 541: | Loss: 0.45229 | Acc: 78.250\n",
      "Epoch 542: | Loss: 0.45214 | Acc: 78.622\n",
      "Epoch 543: | Loss: 0.44787 | Acc: 79.080\n",
      "Epoch 544: | Loss: 0.45558 | Acc: 77.702\n",
      "Epoch 545: | Loss: 0.44951 | Acc: 78.947\n",
      "Epoch 546: | Loss: 0.45620 | Acc: 78.053\n",
      "Epoch 547: | Loss: 0.45245 | Acc: 78.484\n",
      "Epoch 548: | Loss: 0.44827 | Acc: 78.644\n",
      "Epoch 549: | Loss: 0.44798 | Acc: 78.505\n",
      "Epoch 550: | Loss: 0.45459 | Acc: 78.883\n",
      "Epoch 551: | Loss: 0.45360 | Acc: 78.021\n",
      "Epoch 552: | Loss: 0.44963 | Acc: 78.351\n",
      "Epoch 553: | Loss: 0.44940 | Acc: 78.713\n",
      "Epoch 554: | Loss: 0.45464 | Acc: 78.814\n",
      "Epoch 555: | Loss: 0.44587 | Acc: 78.516\n",
      "Epoch 556: | Loss: 0.44568 | Acc: 78.606\n",
      "Epoch 557: | Loss: 0.45027 | Acc: 78.612\n",
      "Epoch 558: | Loss: 0.44816 | Acc: 78.340\n",
      "Epoch 559: | Loss: 0.44833 | Acc: 78.835\n",
      "Epoch 560: | Loss: 0.45434 | Acc: 78.282\n",
      "Epoch 561: | Loss: 0.44777 | Acc: 78.564\n",
      "Epoch 562: | Loss: 0.45270 | Acc: 78.489\n",
      "Epoch 563: | Loss: 0.45338 | Acc: 78.133\n",
      "Epoch 564: | Loss: 0.44969 | Acc: 78.452\n",
      "Epoch 565: | Loss: 0.45008 | Acc: 78.644\n",
      "Epoch 566: | Loss: 0.44619 | Acc: 78.862\n",
      "Epoch 567: | Loss: 0.44929 | Acc: 79.133\n",
      "Epoch 568: | Loss: 0.44000 | Acc: 79.133\n",
      "Epoch 569: | Loss: 0.44691 | Acc: 78.660\n",
      "Epoch 570: | Loss: 0.44806 | Acc: 78.431\n",
      "Epoch 571: | Loss: 0.44480 | Acc: 78.633\n",
      "Epoch 572: | Loss: 0.44771 | Acc: 78.739\n",
      "Epoch 573: | Loss: 0.45465 | Acc: 77.894\n",
      "Epoch 574: | Loss: 0.44695 | Acc: 78.500\n",
      "Epoch 575: | Loss: 0.44439 | Acc: 78.723\n",
      "Epoch 576: | Loss: 0.44854 | Acc: 78.633\n",
      "Epoch 577: | Loss: 0.44450 | Acc: 78.979\n",
      "Epoch 578: | Loss: 0.44823 | Acc: 78.559\n",
      "Epoch 579: | Loss: 0.45192 | Acc: 78.266\n",
      "Epoch 580: | Loss: 0.44913 | Acc: 78.564\n",
      "Epoch 581: | Loss: 0.45047 | Acc: 78.181\n",
      "Epoch 582: | Loss: 0.44654 | Acc: 78.702\n",
      "Epoch 583: | Loss: 0.44240 | Acc: 78.968\n",
      "Epoch 584: | Loss: 0.44371 | Acc: 79.037\n",
      "Epoch 585: | Loss: 0.44048 | Acc: 79.106\n",
      "Epoch 586: | Loss: 0.44293 | Acc: 78.809\n",
      "Epoch 587: | Loss: 0.44247 | Acc: 79.027\n",
      "Epoch 588: | Loss: 0.44025 | Acc: 79.239\n",
      "Epoch 589: | Loss: 0.44512 | Acc: 78.755\n",
      "Epoch 590: | Loss: 0.44735 | Acc: 78.798\n",
      "Epoch 591: | Loss: 0.44575 | Acc: 78.622\n",
      "Epoch 592: | Loss: 0.44425 | Acc: 78.761\n",
      "Epoch 593: | Loss: 0.44147 | Acc: 79.069\n",
      "Epoch 594: | Loss: 0.44790 | Acc: 78.904\n",
      "Epoch 595: | Loss: 0.44142 | Acc: 79.032\n",
      "Epoch 596: | Loss: 0.44128 | Acc: 79.074\n",
      "Epoch 597: | Loss: 0.44982 | Acc: 78.154\n",
      "Epoch 598: | Loss: 0.44312 | Acc: 78.952\n",
      "Epoch 599: | Loss: 0.44976 | Acc: 78.830\n",
      "Epoch 600: | Loss: 0.44826 | Acc: 78.846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601: | Loss: 0.44120 | Acc: 78.872\n",
      "Epoch 602: | Loss: 0.44748 | Acc: 78.532\n",
      "Epoch 603: | Loss: 0.43635 | Acc: 79.122\n",
      "Epoch 604: | Loss: 0.44056 | Acc: 79.128\n",
      "Epoch 605: | Loss: 0.44153 | Acc: 79.128\n",
      "Epoch 606: | Loss: 0.43775 | Acc: 79.500\n",
      "Epoch 607: | Loss: 0.43801 | Acc: 79.457\n",
      "Epoch 608: | Loss: 0.43898 | Acc: 78.947\n",
      "Epoch 609: | Loss: 0.44591 | Acc: 78.851\n",
      "Epoch 610: | Loss: 0.44829 | Acc: 78.872\n",
      "Epoch 611: | Loss: 0.44294 | Acc: 78.957\n",
      "Epoch 612: | Loss: 0.44990 | Acc: 78.473\n",
      "Epoch 613: | Loss: 0.43834 | Acc: 79.122\n",
      "Epoch 614: | Loss: 0.43956 | Acc: 78.856\n",
      "Epoch 615: | Loss: 0.44904 | Acc: 78.516\n",
      "Epoch 616: | Loss: 0.44116 | Acc: 79.043\n",
      "Epoch 617: | Loss: 0.44154 | Acc: 79.133\n",
      "Epoch 618: | Loss: 0.43873 | Acc: 79.516\n",
      "Epoch 619: | Loss: 0.43866 | Acc: 79.495\n",
      "Epoch 620: | Loss: 0.44039 | Acc: 78.777\n",
      "Epoch 621: | Loss: 0.44556 | Acc: 78.968\n",
      "Epoch 622: | Loss: 0.43765 | Acc: 79.122\n",
      "Epoch 623: | Loss: 0.43449 | Acc: 79.548\n",
      "Epoch 624: | Loss: 0.44233 | Acc: 79.154\n",
      "Epoch 625: | Loss: 0.44382 | Acc: 79.138\n",
      "Epoch 626: | Loss: 0.43991 | Acc: 79.128\n",
      "Epoch 627: | Loss: 0.44340 | Acc: 78.926\n",
      "Epoch 628: | Loss: 0.43853 | Acc: 79.080\n",
      "Epoch 629: | Loss: 0.44711 | Acc: 78.580\n",
      "Epoch 630: | Loss: 0.44101 | Acc: 78.840\n",
      "Epoch 631: | Loss: 0.43840 | Acc: 79.080\n",
      "Epoch 632: | Loss: 0.43988 | Acc: 79.074\n",
      "Epoch 633: | Loss: 0.44446 | Acc: 78.734\n",
      "Epoch 634: | Loss: 0.43466 | Acc: 79.628\n",
      "Epoch 635: | Loss: 0.43891 | Acc: 78.883\n",
      "Epoch 636: | Loss: 0.44549 | Acc: 79.043\n",
      "Epoch 637: | Loss: 0.43629 | Acc: 79.106\n",
      "Epoch 638: | Loss: 0.44250 | Acc: 78.968\n",
      "Epoch 639: | Loss: 0.43812 | Acc: 79.798\n",
      "Epoch 640: | Loss: 0.43617 | Acc: 79.979\n",
      "Epoch 641: | Loss: 0.43801 | Acc: 79.223\n",
      "Epoch 642: | Loss: 0.43431 | Acc: 79.282\n",
      "Epoch 643: | Loss: 0.43632 | Acc: 79.096\n",
      "Epoch 644: | Loss: 0.43097 | Acc: 79.617\n",
      "Epoch 645: | Loss: 0.44241 | Acc: 78.824\n",
      "Epoch 646: | Loss: 0.43113 | Acc: 79.824\n",
      "Epoch 647: | Loss: 0.43470 | Acc: 79.404\n",
      "Epoch 648: | Loss: 0.43520 | Acc: 79.309\n",
      "Epoch 649: | Loss: 0.43695 | Acc: 79.729\n",
      "Epoch 650: | Loss: 0.43715 | Acc: 79.218\n",
      "Epoch 651: | Loss: 0.43354 | Acc: 79.335\n",
      "Epoch 652: | Loss: 0.44753 | Acc: 78.681\n",
      "Epoch 653: | Loss: 0.43497 | Acc: 79.548\n",
      "Epoch 654: | Loss: 0.43046 | Acc: 79.585\n",
      "Epoch 655: | Loss: 0.43453 | Acc: 79.495\n",
      "Epoch 656: | Loss: 0.43808 | Acc: 79.340\n",
      "Epoch 657: | Loss: 0.43928 | Acc: 79.372\n",
      "Epoch 658: | Loss: 0.42999 | Acc: 79.505\n",
      "Epoch 659: | Loss: 0.42885 | Acc: 79.697\n",
      "Epoch 660: | Loss: 0.43353 | Acc: 79.505\n",
      "Epoch 661: | Loss: 0.43470 | Acc: 79.649\n",
      "Epoch 662: | Loss: 0.43311 | Acc: 79.346\n",
      "Epoch 663: | Loss: 0.43672 | Acc: 79.074\n",
      "Epoch 664: | Loss: 0.42663 | Acc: 79.755\n",
      "Epoch 665: | Loss: 0.42916 | Acc: 79.915\n",
      "Epoch 666: | Loss: 0.44459 | Acc: 78.989\n",
      "Epoch 667: | Loss: 0.43888 | Acc: 78.851\n",
      "Epoch 668: | Loss: 0.43204 | Acc: 79.702\n",
      "Epoch 669: | Loss: 0.42861 | Acc: 79.410\n",
      "Epoch 670: | Loss: 0.43664 | Acc: 79.532\n",
      "Epoch 671: | Loss: 0.43191 | Acc: 79.585\n",
      "Epoch 672: | Loss: 0.42761 | Acc: 79.718\n",
      "Epoch 673: | Loss: 0.43048 | Acc: 79.452\n",
      "Epoch 674: | Loss: 0.43702 | Acc: 79.309\n",
      "Epoch 675: | Loss: 0.43300 | Acc: 79.904\n",
      "Epoch 676: | Loss: 0.43552 | Acc: 79.883\n",
      "Epoch 677: | Loss: 0.43182 | Acc: 79.702\n",
      "Epoch 678: | Loss: 0.43285 | Acc: 79.638\n",
      "Epoch 679: | Loss: 0.43173 | Acc: 79.521\n",
      "Epoch 680: | Loss: 0.43460 | Acc: 79.394\n",
      "Epoch 681: | Loss: 0.43324 | Acc: 79.149\n",
      "Epoch 682: | Loss: 0.43077 | Acc: 79.457\n",
      "Epoch 683: | Loss: 0.42762 | Acc: 79.824\n",
      "Epoch 684: | Loss: 0.42738 | Acc: 79.888\n",
      "Epoch 685: | Loss: 0.42890 | Acc: 79.734\n",
      "Epoch 686: | Loss: 0.43302 | Acc: 79.819\n",
      "Epoch 687: | Loss: 0.43018 | Acc: 79.505\n",
      "Epoch 688: | Loss: 0.44038 | Acc: 79.064\n",
      "Epoch 689: | Loss: 0.43225 | Acc: 79.734\n",
      "Epoch 690: | Loss: 0.43428 | Acc: 79.819\n",
      "Epoch 691: | Loss: 0.43862 | Acc: 78.984\n",
      "Epoch 692: | Loss: 0.42692 | Acc: 79.468\n",
      "Epoch 693: | Loss: 0.43472 | Acc: 79.585\n",
      "Epoch 694: | Loss: 0.42583 | Acc: 79.840\n",
      "Epoch 695: | Loss: 0.42618 | Acc: 80.154\n",
      "Epoch 696: | Loss: 0.42514 | Acc: 79.580\n",
      "Epoch 697: | Loss: 0.43357 | Acc: 79.638\n",
      "Epoch 698: | Loss: 0.43731 | Acc: 79.569\n",
      "Epoch 699: | Loss: 0.42945 | Acc: 79.920\n",
      "Epoch 700: | Loss: 0.42995 | Acc: 79.835\n",
      "Epoch 701: | Loss: 0.43338 | Acc: 79.633\n",
      "Epoch 702: | Loss: 0.42205 | Acc: 80.021\n",
      "Epoch 703: | Loss: 0.42185 | Acc: 80.527\n",
      "Epoch 704: | Loss: 0.42576 | Acc: 79.782\n",
      "Epoch 705: | Loss: 0.42501 | Acc: 80.335\n",
      "Epoch 706: | Loss: 0.42676 | Acc: 80.250\n",
      "Epoch 707: | Loss: 0.42498 | Acc: 80.149\n",
      "Epoch 708: | Loss: 0.42482 | Acc: 79.723\n",
      "Epoch 709: | Loss: 0.42760 | Acc: 79.718\n",
      "Epoch 710: | Loss: 0.42820 | Acc: 80.005\n",
      "Epoch 711: | Loss: 0.43414 | Acc: 79.303\n",
      "Epoch 712: | Loss: 0.43799 | Acc: 78.835\n",
      "Epoch 713: | Loss: 0.42426 | Acc: 79.766\n",
      "Epoch 714: | Loss: 0.42501 | Acc: 80.080\n",
      "Epoch 715: | Loss: 0.43400 | Acc: 79.888\n",
      "Epoch 716: | Loss: 0.42779 | Acc: 79.862\n",
      "Epoch 717: | Loss: 0.42316 | Acc: 80.085\n",
      "Epoch 718: | Loss: 0.42969 | Acc: 79.782\n",
      "Epoch 719: | Loss: 0.43291 | Acc: 79.527\n",
      "Epoch 720: | Loss: 0.42518 | Acc: 80.064\n",
      "Epoch 721: | Loss: 0.42398 | Acc: 80.202\n",
      "Epoch 722: | Loss: 0.42152 | Acc: 80.043\n",
      "Epoch 723: | Loss: 0.43348 | Acc: 79.367\n",
      "Epoch 724: | Loss: 0.43446 | Acc: 79.468\n",
      "Epoch 725: | Loss: 0.42176 | Acc: 79.973\n",
      "Epoch 726: | Loss: 0.42068 | Acc: 80.218\n",
      "Epoch 727: | Loss: 0.42604 | Acc: 80.176\n",
      "Epoch 728: | Loss: 0.43073 | Acc: 79.750\n",
      "Epoch 729: | Loss: 0.42855 | Acc: 79.644\n",
      "Epoch 730: | Loss: 0.43106 | Acc: 79.516\n",
      "Epoch 731: | Loss: 0.42659 | Acc: 79.745\n",
      "Epoch 732: | Loss: 0.42957 | Acc: 80.000\n",
      "Epoch 733: | Loss: 0.43048 | Acc: 79.840\n",
      "Epoch 734: | Loss: 0.41973 | Acc: 80.229\n",
      "Epoch 735: | Loss: 0.42054 | Acc: 80.149\n",
      "Epoch 736: | Loss: 0.42174 | Acc: 80.362\n",
      "Epoch 737: | Loss: 0.42865 | Acc: 79.559\n",
      "Epoch 738: | Loss: 0.41892 | Acc: 80.628\n",
      "Epoch 739: | Loss: 0.42620 | Acc: 79.718\n",
      "Epoch 740: | Loss: 0.42354 | Acc: 80.277\n",
      "Epoch 741: | Loss: 0.42920 | Acc: 79.915\n",
      "Epoch 742: | Loss: 0.42686 | Acc: 80.154\n",
      "Epoch 743: | Loss: 0.42760 | Acc: 80.027\n",
      "Epoch 744: | Loss: 0.41453 | Acc: 80.362\n",
      "Epoch 745: | Loss: 0.42494 | Acc: 80.277\n",
      "Epoch 746: | Loss: 0.41913 | Acc: 80.559\n",
      "Epoch 747: | Loss: 0.42411 | Acc: 79.846\n",
      "Epoch 748: | Loss: 0.41992 | Acc: 80.293\n",
      "Epoch 749: | Loss: 0.42734 | Acc: 79.851\n",
      "Epoch 750: | Loss: 0.42300 | Acc: 80.181\n",
      "Epoch 751: | Loss: 0.41062 | Acc: 80.883\n",
      "Epoch 752: | Loss: 0.42512 | Acc: 79.851\n",
      "Epoch 753: | Loss: 0.42137 | Acc: 80.447\n",
      "Epoch 754: | Loss: 0.41726 | Acc: 80.202\n",
      "Epoch 755: | Loss: 0.41646 | Acc: 81.000\n",
      "Epoch 756: | Loss: 0.42281 | Acc: 80.239\n",
      "Epoch 757: | Loss: 0.42867 | Acc: 80.032\n",
      "Epoch 758: | Loss: 0.41384 | Acc: 80.564\n",
      "Epoch 759: | Loss: 0.41917 | Acc: 80.548\n",
      "Epoch 760: | Loss: 0.42268 | Acc: 80.426\n",
      "Epoch 761: | Loss: 0.41655 | Acc: 79.899\n",
      "Epoch 762: | Loss: 0.42043 | Acc: 80.388\n",
      "Epoch 763: | Loss: 0.42211 | Acc: 80.309\n",
      "Epoch 764: | Loss: 0.41546 | Acc: 80.324\n",
      "Epoch 765: | Loss: 0.41795 | Acc: 80.176\n",
      "Epoch 766: | Loss: 0.42077 | Acc: 80.197\n",
      "Epoch 767: | Loss: 0.41293 | Acc: 80.862\n",
      "Epoch 768: | Loss: 0.41477 | Acc: 80.835\n",
      "Epoch 769: | Loss: 0.42034 | Acc: 80.032\n",
      "Epoch 770: | Loss: 0.41756 | Acc: 80.633\n",
      "Epoch 771: | Loss: 0.42086 | Acc: 80.138\n",
      "Epoch 772: | Loss: 0.41256 | Acc: 80.463\n",
      "Epoch 773: | Loss: 0.42153 | Acc: 80.138\n",
      "Epoch 774: | Loss: 0.41866 | Acc: 80.495\n",
      "Epoch 775: | Loss: 0.42025 | Acc: 80.330\n",
      "Epoch 776: | Loss: 0.41244 | Acc: 80.910\n",
      "Epoch 777: | Loss: 0.41493 | Acc: 80.505\n",
      "Epoch 778: | Loss: 0.42236 | Acc: 80.739\n",
      "Epoch 779: | Loss: 0.41717 | Acc: 80.920\n",
      "Epoch 780: | Loss: 0.41689 | Acc: 80.574\n",
      "Epoch 781: | Loss: 0.41772 | Acc: 80.894\n",
      "Epoch 782: | Loss: 0.42232 | Acc: 80.213\n",
      "Epoch 783: | Loss: 0.42079 | Acc: 80.234\n",
      "Epoch 784: | Loss: 0.41821 | Acc: 80.862\n",
      "Epoch 785: | Loss: 0.42290 | Acc: 80.213\n",
      "Epoch 786: | Loss: 0.41918 | Acc: 80.277\n",
      "Epoch 787: | Loss: 0.41943 | Acc: 80.128\n",
      "Epoch 788: | Loss: 0.40982 | Acc: 81.245\n",
      "Epoch 789: | Loss: 0.42356 | Acc: 80.255\n",
      "Epoch 790: | Loss: 0.41732 | Acc: 80.271\n",
      "Epoch 791: | Loss: 0.41782 | Acc: 80.702\n",
      "Epoch 792: | Loss: 0.41217 | Acc: 80.713\n",
      "Epoch 793: | Loss: 0.41259 | Acc: 80.330\n",
      "Epoch 794: | Loss: 0.41179 | Acc: 81.096\n",
      "Epoch 795: | Loss: 0.41162 | Acc: 80.484\n",
      "Epoch 796: | Loss: 0.41097 | Acc: 80.718\n",
      "Epoch 797: | Loss: 0.41006 | Acc: 81.106\n",
      "Epoch 798: | Loss: 0.41987 | Acc: 80.335\n",
      "Epoch 799: | Loss: 0.41733 | Acc: 80.505\n",
      "Epoch 800: | Loss: 0.41598 | Acc: 80.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 801: | Loss: 0.41236 | Acc: 80.915\n",
      "Epoch 802: | Loss: 0.41049 | Acc: 80.867\n",
      "Epoch 803: | Loss: 0.41450 | Acc: 80.559\n",
      "Epoch 804: | Loss: 0.41547 | Acc: 80.431\n",
      "Epoch 805: | Loss: 0.41817 | Acc: 80.261\n",
      "Epoch 806: | Loss: 0.40993 | Acc: 81.250\n",
      "Epoch 807: | Loss: 0.40942 | Acc: 80.777\n",
      "Epoch 808: | Loss: 0.41829 | Acc: 80.628\n",
      "Epoch 809: | Loss: 0.41438 | Acc: 80.564\n",
      "Epoch 810: | Loss: 0.41379 | Acc: 81.037\n",
      "Epoch 811: | Loss: 0.41320 | Acc: 80.952\n",
      "Epoch 812: | Loss: 0.41817 | Acc: 80.532\n",
      "Epoch 813: | Loss: 0.41384 | Acc: 80.351\n",
      "Epoch 814: | Loss: 0.41264 | Acc: 80.691\n",
      "Epoch 815: | Loss: 0.41305 | Acc: 80.936\n",
      "Epoch 816: | Loss: 0.41111 | Acc: 80.745\n",
      "Epoch 817: | Loss: 0.40980 | Acc: 80.867\n",
      "Epoch 818: | Loss: 0.41539 | Acc: 80.697\n",
      "Epoch 819: | Loss: 0.40909 | Acc: 80.723\n",
      "Epoch 820: | Loss: 0.41677 | Acc: 80.537\n",
      "Epoch 821: | Loss: 0.40805 | Acc: 81.149\n",
      "Epoch 822: | Loss: 0.41502 | Acc: 80.367\n",
      "Epoch 823: | Loss: 0.39999 | Acc: 81.101\n",
      "Epoch 824: | Loss: 0.40596 | Acc: 80.878\n",
      "Epoch 825: | Loss: 0.41033 | Acc: 80.920\n",
      "Epoch 826: | Loss: 0.40991 | Acc: 80.952\n",
      "Epoch 827: | Loss: 0.40945 | Acc: 80.984\n",
      "Epoch 828: | Loss: 0.40589 | Acc: 81.117\n",
      "Epoch 829: | Loss: 0.40811 | Acc: 81.149\n",
      "Epoch 830: | Loss: 0.40619 | Acc: 81.053\n",
      "Epoch 831: | Loss: 0.40780 | Acc: 81.112\n",
      "Epoch 832: | Loss: 0.41958 | Acc: 80.287\n",
      "Epoch 833: | Loss: 0.41984 | Acc: 80.601\n",
      "Epoch 834: | Loss: 0.41066 | Acc: 80.761\n",
      "Epoch 835: | Loss: 0.41015 | Acc: 81.090\n",
      "Epoch 836: | Loss: 0.40662 | Acc: 81.303\n",
      "Epoch 837: | Loss: 0.41221 | Acc: 80.819\n",
      "Epoch 838: | Loss: 0.40828 | Acc: 80.995\n",
      "Epoch 839: | Loss: 0.40889 | Acc: 80.840\n",
      "Epoch 840: | Loss: 0.40951 | Acc: 80.979\n",
      "Epoch 841: | Loss: 0.40424 | Acc: 80.979\n",
      "Epoch 842: | Loss: 0.41281 | Acc: 80.654\n",
      "Epoch 843: | Loss: 0.41343 | Acc: 80.814\n",
      "Epoch 844: | Loss: 0.41073 | Acc: 81.277\n",
      "Epoch 845: | Loss: 0.41308 | Acc: 80.867\n",
      "Epoch 846: | Loss: 0.41759 | Acc: 80.293\n",
      "Epoch 847: | Loss: 0.41683 | Acc: 80.548\n",
      "Epoch 848: | Loss: 0.40367 | Acc: 81.122\n",
      "Epoch 849: | Loss: 0.40873 | Acc: 80.984\n",
      "Epoch 850: | Loss: 0.41639 | Acc: 80.484\n",
      "Epoch 851: | Loss: 0.40906 | Acc: 81.149\n",
      "Epoch 852: | Loss: 0.40975 | Acc: 81.266\n",
      "Epoch 853: | Loss: 0.40389 | Acc: 81.059\n",
      "Epoch 854: | Loss: 0.41399 | Acc: 80.947\n",
      "Epoch 855: | Loss: 0.40931 | Acc: 80.872\n",
      "Epoch 856: | Loss: 0.40925 | Acc: 80.856\n",
      "Epoch 857: | Loss: 0.40630 | Acc: 81.356\n",
      "Epoch 858: | Loss: 0.40128 | Acc: 81.261\n",
      "Epoch 859: | Loss: 0.40376 | Acc: 80.915\n",
      "Epoch 860: | Loss: 0.40504 | Acc: 81.080\n",
      "Epoch 861: | Loss: 0.40416 | Acc: 81.213\n",
      "Epoch 862: | Loss: 0.40471 | Acc: 81.048\n",
      "Epoch 863: | Loss: 0.41531 | Acc: 80.670\n",
      "Epoch 864: | Loss: 0.41085 | Acc: 81.133\n",
      "Epoch 865: | Loss: 0.40312 | Acc: 81.527\n",
      "Epoch 866: | Loss: 0.40713 | Acc: 81.000\n",
      "Epoch 867: | Loss: 0.40920 | Acc: 80.628\n",
      "Epoch 868: | Loss: 0.40767 | Acc: 81.362\n",
      "Epoch 869: | Loss: 0.40629 | Acc: 81.027\n",
      "Epoch 870: | Loss: 0.40606 | Acc: 81.452\n",
      "Epoch 871: | Loss: 0.40064 | Acc: 81.559\n",
      "Epoch 872: | Loss: 0.40944 | Acc: 81.064\n",
      "Epoch 873: | Loss: 0.40631 | Acc: 81.383\n",
      "Epoch 874: | Loss: 0.41016 | Acc: 80.846\n",
      "Epoch 875: | Loss: 0.40518 | Acc: 81.559\n",
      "Epoch 876: | Loss: 0.39898 | Acc: 81.335\n",
      "Epoch 877: | Loss: 0.40324 | Acc: 81.335\n",
      "Epoch 878: | Loss: 0.39841 | Acc: 81.771\n",
      "Epoch 879: | Loss: 0.40116 | Acc: 81.287\n",
      "Epoch 880: | Loss: 0.41203 | Acc: 80.420\n",
      "Epoch 881: | Loss: 0.40538 | Acc: 81.266\n",
      "Epoch 882: | Loss: 0.40613 | Acc: 81.383\n",
      "Epoch 883: | Loss: 0.39809 | Acc: 81.750\n",
      "Epoch 884: | Loss: 0.40669 | Acc: 81.085\n",
      "Epoch 885: | Loss: 0.40835 | Acc: 81.085\n",
      "Epoch 886: | Loss: 0.40117 | Acc: 81.324\n",
      "Epoch 887: | Loss: 0.40437 | Acc: 81.298\n",
      "Epoch 888: | Loss: 0.40518 | Acc: 81.138\n",
      "Epoch 889: | Loss: 0.40364 | Acc: 81.250\n",
      "Epoch 890: | Loss: 0.40326 | Acc: 81.165\n",
      "Epoch 891: | Loss: 0.40383 | Acc: 81.356\n",
      "Epoch 892: | Loss: 0.40522 | Acc: 81.261\n",
      "Epoch 893: | Loss: 0.40238 | Acc: 81.580\n",
      "Epoch 894: | Loss: 0.40717 | Acc: 81.011\n",
      "Epoch 895: | Loss: 0.40949 | Acc: 80.713\n",
      "Epoch 896: | Loss: 0.40396 | Acc: 80.941\n",
      "Epoch 897: | Loss: 0.39871 | Acc: 81.559\n",
      "Epoch 898: | Loss: 0.39980 | Acc: 81.505\n",
      "Epoch 899: | Loss: 0.39904 | Acc: 81.883\n",
      "Epoch 900: | Loss: 0.40297 | Acc: 81.250\n",
      "Epoch 901: | Loss: 0.40087 | Acc: 81.676\n",
      "Epoch 902: | Loss: 0.40587 | Acc: 81.122\n",
      "Epoch 903: | Loss: 0.39176 | Acc: 82.266\n",
      "Epoch 904: | Loss: 0.39623 | Acc: 81.739\n",
      "Epoch 905: | Loss: 0.40657 | Acc: 81.309\n",
      "Epoch 906: | Loss: 0.39725 | Acc: 81.628\n",
      "Epoch 907: | Loss: 0.40541 | Acc: 81.362\n",
      "Epoch 908: | Loss: 0.40375 | Acc: 81.346\n",
      "Epoch 909: | Loss: 0.39588 | Acc: 81.537\n",
      "Epoch 910: | Loss: 0.39448 | Acc: 81.723\n",
      "Epoch 911: | Loss: 0.40475 | Acc: 81.213\n",
      "Epoch 912: | Loss: 0.40139 | Acc: 81.324\n",
      "Epoch 913: | Loss: 0.39332 | Acc: 82.080\n",
      "Epoch 914: | Loss: 0.39209 | Acc: 82.021\n",
      "Epoch 915: | Loss: 0.39544 | Acc: 81.660\n",
      "Epoch 916: | Loss: 0.39756 | Acc: 81.660\n",
      "Epoch 917: | Loss: 0.39078 | Acc: 82.064\n",
      "Epoch 918: | Loss: 0.40000 | Acc: 81.303\n",
      "Epoch 919: | Loss: 0.40397 | Acc: 81.250\n",
      "Epoch 920: | Loss: 0.39756 | Acc: 81.580\n",
      "Epoch 921: | Loss: 0.40478 | Acc: 81.436\n",
      "Epoch 922: | Loss: 0.39846 | Acc: 81.622\n",
      "Epoch 923: | Loss: 0.39444 | Acc: 81.867\n",
      "Epoch 924: | Loss: 0.39510 | Acc: 82.117\n",
      "Epoch 925: | Loss: 0.39644 | Acc: 81.676\n",
      "Epoch 926: | Loss: 0.39981 | Acc: 81.202\n",
      "Epoch 927: | Loss: 0.39740 | Acc: 81.527\n",
      "Epoch 928: | Loss: 0.39785 | Acc: 81.553\n",
      "Epoch 929: | Loss: 0.39342 | Acc: 81.782\n",
      "Epoch 930: | Loss: 0.39249 | Acc: 81.883\n",
      "Epoch 931: | Loss: 0.39734 | Acc: 81.617\n",
      "Epoch 932: | Loss: 0.39220 | Acc: 81.899\n",
      "Epoch 933: | Loss: 0.39528 | Acc: 81.612\n",
      "Epoch 934: | Loss: 0.40082 | Acc: 80.867\n",
      "Epoch 935: | Loss: 0.39568 | Acc: 81.947\n",
      "Epoch 936: | Loss: 0.39869 | Acc: 81.771\n",
      "Epoch 937: | Loss: 0.40026 | Acc: 81.936\n",
      "Epoch 938: | Loss: 0.39052 | Acc: 82.239\n",
      "Epoch 939: | Loss: 0.39621 | Acc: 81.441\n",
      "Epoch 940: | Loss: 0.39546 | Acc: 81.537\n",
      "Epoch 941: | Loss: 0.39213 | Acc: 82.043\n",
      "Epoch 942: | Loss: 0.39434 | Acc: 81.995\n",
      "Epoch 943: | Loss: 0.39489 | Acc: 81.745\n",
      "Epoch 944: | Loss: 0.40005 | Acc: 81.995\n",
      "Epoch 945: | Loss: 0.39562 | Acc: 81.793\n",
      "Epoch 946: | Loss: 0.39248 | Acc: 82.059\n",
      "Epoch 947: | Loss: 0.40590 | Acc: 81.154\n",
      "Epoch 948: | Loss: 0.38906 | Acc: 82.154\n",
      "Epoch 949: | Loss: 0.38923 | Acc: 81.814\n",
      "Epoch 950: | Loss: 0.39661 | Acc: 81.851\n",
      "Epoch 951: | Loss: 0.39051 | Acc: 81.931\n",
      "Epoch 952: | Loss: 0.39296 | Acc: 82.261\n",
      "Epoch 953: | Loss: 0.39020 | Acc: 82.122\n",
      "Epoch 954: | Loss: 0.38605 | Acc: 82.468\n",
      "Epoch 955: | Loss: 0.39344 | Acc: 82.101\n",
      "Epoch 956: | Loss: 0.39859 | Acc: 81.665\n",
      "Epoch 957: | Loss: 0.38927 | Acc: 82.351\n",
      "Epoch 958: | Loss: 0.39360 | Acc: 81.729\n",
      "Epoch 959: | Loss: 0.39812 | Acc: 81.989\n",
      "Epoch 960: | Loss: 0.38772 | Acc: 82.005\n",
      "Epoch 961: | Loss: 0.39437 | Acc: 81.564\n",
      "Epoch 962: | Loss: 0.39236 | Acc: 82.245\n",
      "Epoch 963: | Loss: 0.38698 | Acc: 82.122\n",
      "Epoch 964: | Loss: 0.39605 | Acc: 82.218\n",
      "Epoch 965: | Loss: 0.38787 | Acc: 82.239\n",
      "Epoch 966: | Loss: 0.38712 | Acc: 81.872\n",
      "Epoch 967: | Loss: 0.39125 | Acc: 82.122\n",
      "Epoch 968: | Loss: 0.39904 | Acc: 81.729\n",
      "Epoch 969: | Loss: 0.38989 | Acc: 82.186\n",
      "Epoch 970: | Loss: 0.39199 | Acc: 81.750\n",
      "Epoch 971: | Loss: 0.38617 | Acc: 82.665\n",
      "Epoch 972: | Loss: 0.38911 | Acc: 82.250\n",
      "Epoch 973: | Loss: 0.38176 | Acc: 82.436\n",
      "Epoch 974: | Loss: 0.39348 | Acc: 81.723\n",
      "Epoch 975: | Loss: 0.39260 | Acc: 82.122\n",
      "Epoch 976: | Loss: 0.39216 | Acc: 82.207\n",
      "Epoch 977: | Loss: 0.39518 | Acc: 81.261\n",
      "Epoch 978: | Loss: 0.38841 | Acc: 82.314\n",
      "Epoch 979: | Loss: 0.38874 | Acc: 82.351\n",
      "Epoch 980: | Loss: 0.38829 | Acc: 82.011\n",
      "Epoch 981: | Loss: 0.39208 | Acc: 81.952\n",
      "Epoch 982: | Loss: 0.38840 | Acc: 82.138\n",
      "Epoch 983: | Loss: 0.39365 | Acc: 81.691\n",
      "Epoch 984: | Loss: 0.38071 | Acc: 82.426\n",
      "Epoch 985: | Loss: 0.38995 | Acc: 82.261\n",
      "Epoch 986: | Loss: 0.38691 | Acc: 81.888\n",
      "Epoch 987: | Loss: 0.39172 | Acc: 81.729\n",
      "Epoch 988: | Loss: 0.39191 | Acc: 82.080\n",
      "Epoch 989: | Loss: 0.38676 | Acc: 82.186\n",
      "Epoch 990: | Loss: 0.39700 | Acc: 81.644\n",
      "Epoch 991: | Loss: 0.38923 | Acc: 82.021\n",
      "Epoch 992: | Loss: 0.38558 | Acc: 82.388\n",
      "Epoch 993: | Loss: 0.39448 | Acc: 81.649\n",
      "Epoch 994: | Loss: 0.38580 | Acc: 82.319\n",
      "Epoch 995: | Loss: 0.38679 | Acc: 82.202\n",
      "Epoch 996: | Loss: 0.38525 | Acc: 82.489\n",
      "Epoch 997: | Loss: 0.38713 | Acc: 82.202\n",
      "Epoch 998: | Loss: 0.38966 | Acc: 82.340\n",
      "Epoch 999: | Loss: 0.39108 | Acc: 81.936\n",
      "Epoch 1000: | Loss: 0.38067 | Acc: 82.707\n"
     ]
    }
   ],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "train_loss = []\n",
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        train_loss.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_dataloader):.5f} | Acc: {epoch_acc/len(train_dataloader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "137b105f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c36f5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_prob_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_prob = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_prob)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_prob_list.append(y_test_pred.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_prob_list = [a.squeeze().tolist() for a in y_prob_list]\n",
    "y_pred_list = np.concatenate(y_pred_list).tolist()\n",
    "y_prob_list = np.concatenate(y_prob_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89071375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7345661033409543"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f9743dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.66      0.69      1321\n",
      "         1.0       0.75      0.81      0.78      1679\n",
      "\n",
      "    accuracy                           0.74      3000\n",
      "   macro avg       0.74      0.73      0.74      3000\n",
      "weighted avg       0.74      0.74      0.74      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77a5db11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1\n",
       "2995 1.00 0.99\n",
       "2996 1.00 0.56\n",
       "2997 1.00 0.92\n",
       "2998 0.00 0.48\n",
       "2999 0.00 0.01"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answer = pd.DataFrame([ x for x in zip(y_pred_list, y_prob_list)])\n",
    "df_answer.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977f9a6",
   "metadata": {},
   "source": [
    "### Test for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88d68be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0000,  1.0000,  1.0000,  ...,  6.0000, 13.0000,  0.0000],\n",
       "        [ 4.0000,  4.0000,  1.0000,  ...,  5.5000, 10.0000,  1.0000],\n",
       "        [ 4.0000,  5.0000,  1.0000,  ...,  4.0000, 10.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 4.0000,  5.0000,  1.0000,  ...,  5.5000, 10.0000,  0.0000],\n",
       "        [ 4.0000,  4.0000,  1.0000,  ...,  4.5000, 11.0000,  0.0000],\n",
       "        [ 3.0000,  3.0000,  3.0000,  ...,  5.5000,  9.0000,  1.0000]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor(test_X.to_numpy(), dtype=torch.float32)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8116006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(test)\n",
    "prediction_prob = torch.sigmoid(prediction)\n",
    "prediction = torch.round(prediction_prob)\n",
    "prediction = [a.squeeze().tolist() for a in prediction]\n",
    "prediction_prob = [a.squeeze().tolist() for a in prediction_prob]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cfb4b",
   "metadata": {},
   "source": [
    "# 6. Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "ee875238",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = model # you should fill out this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "7552ad6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nn_yesQ_0809_18_18'"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "filename = datetime.datetime.now().strftime(\"%m%d_%H_%M\")\n",
    "filename = 'nn_yesQ_'+filename\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "f71d8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, path+filename+'.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
